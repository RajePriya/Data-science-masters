{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7114220",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f44f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. You are working on a machine learning project where you have a dataset containing numerical and\n",
    "categorical features. You have identified that some of the features are highly correlated and there are\n",
    "missing values in some of the columns. You want to build a pipeline that automates the feature\n",
    "engineering process and handles the missing valuesD\n",
    "Design a pipeline that includes the following steps\"\n",
    "Use an automated feature selection method to identify the important features in the datasetC\n",
    "Create a numerical pipeline that includes the following steps\"\n",
    "Impute the missing values in the numerical columns using the mean of the column valuesC\n",
    "Scale the numerical columns using standardisationC\n",
    "Create a categorical pipeline that includes the following steps\"\n",
    "Impute the missing values in the categorical columns usng the most frequent value of the columnC\n",
    "One-hot encode the categorical columnsC\n",
    "Combine the numerical and categorical pipelines using a ColumnTransformerC\n",
    "Use a Random Forest Classifier to build the final modelC\n",
    "Evaluate the accuracy of the model on the test datasetD\n",
    "Note! Your solution should include code snippets for each step of the pipeline, and a brief explanation of\n",
    "each step. You should also provide an interpretation of the results and suggest possible improvements for\n",
    "the pipelineD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e3c2d7",
   "metadata": {},
   "source": [
    "Ans:-\n",
    "\n",
    "To create a pipeline that automates the feature engineering process and handles missing values, we can follow the steps you mentioned. Let's break down the process step by step and provide code snippets for each part:\n",
    "\n",
    "Automated Feature Selection:\n",
    "We can use feature selection techniques like SelectKBest or SelectPercentile from scikit-learn to identify the important features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39858b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_base.py:49: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode = stats.mode(array)\n",
      "C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:170: UserWarning: Found unknown categories in columns [0, 1, 2, 3, 4] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5333333333333333\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a synthetic dataset with 100 samples and 10 features (5 numerical and 5 categorical)\n",
    "X, y = make_classification(n_samples=100, n_features=10, n_informative=5, random_state=42)\n",
    "\n",
    "# Preprocess the entire dataset (numerical and categorical columns)\n",
    "numerical_features = [0, 1, 2, 3, 4]  # Assuming the first 5 features are numerical\n",
    "categorical_features = [5, 6, 7, 8, 9]  # Assuming the last 5 features are categorical\n",
    "\n",
    "# Numerical Pipeline\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical Pipeline\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine Numerical and Categorical Pipelines\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "])\n",
    "\n",
    "# Create the Random Forest Classifier\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Combine Preprocessor and Classifier in a Final Pipeline\n",
    "final_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', clf)\n",
    "])\n",
    "\n",
    "# Split the preprocessed data into training and test sets (70% training, 30% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = final_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee03156b",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2de031",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Build a pipeline that includes a random forest classifier and a logistic regression classifer, and then\n",
    "use a voting classifier to combine their predictions. Train the pipeline on the iris dataset and evaluate its\n",
    "accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "612aec53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets (70% training, 30% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create the Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create the Logistic Regression Classifier\n",
    "lr_clf = LogisticRegression(random_state=42)\n",
    "\n",
    "# Create the Voting Classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('rf', rf_clf), ('lr', lr_clf)],\n",
    "    voting='hard'  # Use 'hard' voting, where the majority class wins\n",
    ")\n",
    "\n",
    "# Build the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Optionally scale the features\n",
    "    ('voting', voting_clf)\n",
    "])\n",
    "\n",
    "# Train the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e4f77b",
   "metadata": {},
   "source": [
    "To build a pipeline that includes both a Random Forest Classifier and a Logistic Regression Classifier, and then use a Voting Classifier to combine their predictions, we can use scikit-learn's Pipeline and VotingClassifier. The Iris dataset is a well-known dataset, and scikit-learn provides an easy way to load it. Let's proceed with the implementation:\n",
    "\n",
    "we first load the Iris dataset using load_iris() from scikit-learn. We then split the data into training and testing sets. Next, we create the Random Forest Classifier (rf_clf) and the Logistic Regression Classifier (lr_clf). The Voting Classifier (voting_clf) combines these two classifiers using hard voting, where the majority class among the predictions is chosen.\n",
    "\n",
    "We then create a pipeline (pipeline) that includes the StandardScaler to scale the features (optional) and the Voting Classifier.\n",
    "\n",
    "Finally, we train the pipeline on the training data and evaluate its accuracy on the test data.\n",
    "\n",
    "Please note that the dataset used in this example is small, and it's often recommended to have larger datasets for reliable evaluations. In a real-world scenario, you can replace the Iris dataset with your dataset using appropriate data loading methods like pd.read_csv() or any other method suitable for your data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc55f971",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
