{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "305e9254-1ab5-40e5-a03e-e0d0c3f55fe1",
   "metadata": {},
   "source": [
    "## 17MAR\n",
    "### Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e4773f-2fb0-46db-92ad-d3b70b9a1ef5",
   "metadata": {},
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182ccb35-6bf2-46f2-a279-8e9be8749f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddd4ca0-4016-43c4-ad3d-56431c0bdf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:- Missing values in a dataset refer to the absence of a particular value in a variable for a specific observation or \n",
    "data point. Missing values can occur due to various reasons, such as incomplete data collection, data entry errors, or\n",
    "non-response to a survey question.\n",
    "\n",
    "Handling missing values is crucial because they can adversely affect the accuracy and validity of data analysis results. If \n",
    "missing values are not handled properly, they can lead to biased or incomplete analysis, which can result in incorrect \n",
    "conclusions and decisions.\n",
    "\n",
    "Some of the reasons why handling missing values is essential are:\n",
    "\n",
    "=> Missing values can result in reduced sample size, which can lead to a loss of statistical power.\n",
    "\n",
    "=> Ignoring missing values can lead to biased estimates of model parameters and relationships between variables.\n",
    "\n",
    "=> The presence of missing values can affect the results of certain statistical analyses, such as correlation, regression,\n",
    "and factor analysis.\n",
    "\n",
    "=> Missing values can cause errors in data analysis and modeling, such as multicollinearity and overfitting.\n",
    "\n",
    "Some of the algorithms that are not affected by missing values include:\n",
    "\n",
    "=> Decision trees: Decision tree algorithms can handle missing values by replacing them with surrogate splits based on other\n",
    "variables.\n",
    "\n",
    "=> Random forests: Random forest algorithms can handle missing values by using multiple decision trees and averaging the \n",
    "results.\n",
    "\n",
    "=> K-nearest neighbors: K-nearest neighbors algorithms can handle missing values by imputing missing values based on the\n",
    "values of the nearest neighbors.\n",
    "\n",
    "=> Support vector machines: Support vector machine algorithms can handle missing values by using a kernel function that does\n",
    "not require complete data.\n",
    "\n",
    "=> Naive Bayes: Naive Bayes algorithms can handle missing values by ignoring them and calculating probabilities based on\n",
    "available data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e672faba-fd1f-4f38-833e-6bc5a07825e1",
   "metadata": {},
   "source": [
    "### Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f890dab-2a1b-44b5-bb0a-1fc7fb7f7328",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67752f29-2cca-4668-a0ba-111162c4eae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:- There are various techniques used to handle missing data. Here are some common techniques along with examples of \n",
    "Python code:\n",
    "\n",
    "=> Deleting rows or columns with missing data:\n",
    "One approach to handling missing data is to remove any observations or variables with missing values. However, this \n",
    "technique should be used with caution as it can lead to a loss of valuable data.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc5d164d-01d1-476a-8004-f0ff72a37b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B     C\n",
      "1  2.0   7.0  12.0\n",
      "4  5.0  10.0  15.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create a sample dataframe with missing data\n",
    "data = {'A': [1, 2, None, 4, 5],\n",
    "        'B': [None, 7, 8, 9, 10],\n",
    "        'C': [11, 12, 13, None, 15]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# drop rows with missing data\n",
    "df_clean = df.dropna()\n",
    "print(df_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00475bb7-8f27-453c-8c09-ba6d879bfaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "=> Imputing missing data with mean, median or mode:\n",
    "Another technique to handle missing data is to replace missing values with the mean, median or mode of the variable. This \n",
    "technique can be useful when the amount of missing data is relatively small.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9da31bfd-5942-404a-9c03-1bcbec9eb9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B      C\n",
      "0  1.0   8.5  11.00\n",
      "1  2.0   7.0  12.00\n",
      "2  3.0   8.0  13.00\n",
      "3  4.0   9.0  12.75\n",
      "4  5.0  10.0  15.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# create a sample dataframe with missing data\n",
    "data = {'A': [1, 2, None, 4, 5],\n",
    "        'B': [None, 7, 8, 9, 10],\n",
    "        'C': [11, 12, 13, None, 15]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# impute missing values with mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df))\n",
    "df_imputed.columns = df.columns\n",
    "df_imputed.index = df.index\n",
    "print(df_imputed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaded86d-9134-4029-ae76-e3b5498d60c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "=> Imputing missing data with a machine learning algorithm:\n",
    "Another technique to handle missing data is to use machine learning algorithms to predict missing values based on the values\n",
    "of other variables in the dataset. This technique can be useful when there are complex relationships between variables.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d0fc08b-1d04-4ce1-97c3-700e2b8ca1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      A      B      C\n",
      "0  1.00   7.36  11.00\n",
      "1  2.00   7.00  12.00\n",
      "2  2.25   8.00  13.00\n",
      "3  4.00   9.00  13.91\n",
      "4  5.00  10.00  15.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/impute/_iterative.py:785: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# create a sample dataframe with missing data\n",
    "data = {'A': [1, 2, None, 4, 5],\n",
    "        'B': [None, 7, 8, 9, 10],\n",
    "        'C': [11, 12, 13, None, 15]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# impute missing values with iterative imputer\n",
    "imputer = IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=0)\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df))\n",
    "df_imputed.columns = df.columns\n",
    "df_imputed.index = df.index\n",
    "print(df_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d05a9b-cff7-4462-8f36-a360525dcdb1",
   "metadata": {},
   "source": [
    "### Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22007b2-3f55-4778-b8ec-3e1f1c3fb5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88be9e2e-a620-4530-9de3-095fef31a1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:- Imbalanced data refers to a situation where the distribution of classes in a dataset is not equal. This means that the\n",
    "number of observations or data points in one class is significantly higher or lower than the number of observations in the \n",
    "other class(es).\n",
    "\n",
    "For example, consider a binary classification problem where 95% of the observations belong to class A and only 5% belong to \n",
    "class B. This is an example of imbalanced data.\n",
    "\n",
    "If imbalanced data is not handled, it can lead to biased or inaccurate predictions, especially for the minority class. This\n",
    "is because most machine learning algorithms are designed to maximize overall accuracy and may not consider the imbalance in\n",
    "the data.\n",
    "\n",
    "For instance, if we use an imbalanced dataset to train a classifier, it may tend to predict the majority class in most cases,\n",
    "resulting in low accuracy for the minority class. This is because the classifier is optimized to minimize the overall error,\n",
    "and the minority class may be treated as noise and ignored.\n",
    "\n",
    "Moreover, imbalanced data can lead to overfitting, which occurs when the model becomes too complex and starts to memorize \n",
    "the training data rather than learning from it. This can cause poor generalization to new data and reduce the model's \n",
    "performance.\n",
    "\n",
    "Therefore, handling imbalanced data is crucial to ensure that the machine learning algorithm can learn from all the \n",
    "available data and make accurate predictions for all classes. Some techniques to handle imbalanced data include oversampling\n",
    "the minority class, undersampling the majority class, and using specialized algorithms such as ensemble methods, boosting, \n",
    "or bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad93613-79f3-4698-ade2-476df9556fab",
   "metadata": {},
   "source": [
    "### Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cbd970-3795-4571-8022-0c602598f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83933416-faa1-43b7-a576-e6cc55ececa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:- Up-sampling and down-sampling are two common techniques used to handle imbalanced data.\n",
    "\n",
    "Up-sampling refers to the process of increasing the number of observations in the minority class by randomly duplicating \n",
    "existing observations. This technique can be useful when the number of observations in the minority class is significantly\n",
    "smaller than the majority class.\n",
    "\n",
    "For example, suppose we have a dataset with two classes - Class A and Class B - and the number of observations in Class B is \n",
    "much smaller than Class A. In this case, we can use up-sampling to increase the number of observations in Class B to balance\n",
    "the dataset.\n",
    "\n",
    "Down-sampling, on the other hand, refers to the process of reducing the number of observations in the majority class by \n",
    "randomly removing observations. This technique can be useful when the number of observations in the majority class is \n",
    "significantly larger than the minority class.\n",
    "\n",
    "For example, suppose we have a dataset with two classes - Class A and Class B - and the number of observations in Class A \n",
    "is much larger than Class B. In this case, we can use down-sampling to reduce the number of observations in Class A to \n",
    "balance the dataset.\n",
    "\n",
    "Both up-sampling and down-sampling have their advantages and disadvantages, and the choice of technique depends on the \n",
    "specific problem and dataset.\n",
    "\n",
    "For example, suppose we have a dataset with 100 observations, of which 80 belong to Class A and 20 belong to Class B. In \n",
    "this case, the dataset is imbalanced, and we need to balance the classes to ensure that our machine learning algorithm can \n",
    "learn from all the available data and make accurate predictions for both classes.\n",
    "\n",
    "If we choose to use up-sampling, we can duplicate the 20 observations in Class B to create a new dataset with 40 observations\n",
    "in Class B and 80 observations in Class A. If we choose to use down-sampling, we can randomly remove 60 observations from \n",
    "Class A to create a new dataset with 20 observations in Class B and 20 observations in Class A.\n",
    "\n",
    "The choice of technique depends on various factors such as the size of the dataset, the number of classes, the distribution\n",
    "of classes, and the performance of the machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f372f663-4ac9-44e9-99b0-1df7e4f5cfdf",
   "metadata": {},
   "source": [
    "### Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa6a0cc-ad6d-4a03-89a2-960b36d50c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5: What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7b1fbc-b5d9-4ca8-80ee-46831e4878ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:- Data augmentation is a technique used to increase the size of a dataset by creating new examples from existing data. \n",
    "This technique is commonly used in machine learning and deep learning to prevent overfitting, improve model performance,\n",
    "and handle imbalanced data.\n",
    "\n",
    "One popular method for data augmentation is SMOTE (Synthetic Minority Over-sampling Technique). SMOTE creates synthetic\n",
    "examples for the minority class by selecting a random example from the minority class and then finding its k nearest \n",
    "neighbors. The synthetic examples are then created by interpolating between the randomly selected example and its k nearest\n",
    "neighbors.\n",
    "\n",
    "For example, suppose we have a dataset with two classes - Class A and Class B - and the number of observations in Class B\n",
    "is much smaller than Class A. In this case, we can use SMOTE to create synthetic examples for Class B by interpolating \n",
    "between existing examples.\n",
    "\n",
    "The SMOTE algorithm works as follows:\n",
    "\n",
    "=> Select a random example from the minority class.\n",
    "=> Find its k nearest neighbors in the feature space.\n",
    "=> For each of the k nearest neighbors, create a new example by interpolating between the random example and the neighbor.\n",
    "The interpolation is done by choosing a random point on the line segment that connects the two examples.\n",
    "=> Repeat steps 1-3 until the desired number of synthetic examples is created.\n",
    "The SMOTE algorithm can be customized by specifying the number of neighbors (k) and the ratio of synthetic examples to \n",
    "create. SMOTE is effective in handling imbalanced datasets, especially when combined with other techniques such as\n",
    "undersampling or oversampling.\n",
    "\n",
    "One advantage of SMOTE is that it creates synthetic examples that are more representative of the minority class than simply\n",
    "duplicating existing examples. This can improve the generalization of the machine learning model and prevent overfitting.\n",
    "\n",
    "However, SMOTE can also introduce noise into the dataset if the interpolation is not done carefully. Therefore, it is \n",
    "important to choose the right values for k and the ratio of synthetic examples and carefully evaluate the performance of the\n",
    "machine learning model after applying SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724a6f7e-ccdd-4f67-bb1e-4eb0d29f12fe",
   "metadata": {},
   "source": [
    "### Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82a8075-8d98-447d-b338-8396203b2009",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d45838-230c-483c-a90d-49278ea00814",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:- Outliers are data points that are significantly different from the other data points in a dataset. They can be caused \n",
    "by errors in data collection, measurement errors, or extreme events. Outliers can have a significant impact on the analysis \n",
    "and interpretation of data, and can result in incorrect conclusions and models.\n",
    "\n",
    "It is essential to handle outliers because they can lead to biased estimates of model parameters and reduce the accuracy and\n",
    "reliability of statistical models. Outliers can also affect the normal distribution of data, which is a fundamental \n",
    "assumption of many statistical models.\n",
    "\n",
    "Handling outliers involves identifying them and deciding how to treat them. There are various techniques for handling \n",
    "outliers, including:\n",
    "\n",
    "=> Removing the outliers: One approach is to remove the outliers from the dataset. However, this can result in a loss of\n",
    "information and reduction in the size of the dataset. Therefore, it is essential to carefully evaluate the impact of \n",
    "removing outliers on the analysis and model performance.\n",
    "\n",
    "=> Transforming the data: Another approach is to transform the data to reduce the impact of outliers. This can be done by\n",
    "using non-linear transformations such as log or square root transformation, which can reduce the influence of extreme values.\n",
    "\n",
    "=> Winsorizing the data: Winsorizing is a technique that involves replacing the extreme values with less extreme values. \n",
    "This can be done by replacing the extreme values with the nearest values that are not outliers.\n",
    "\n",
    "=> Using robust statistical models: Robust statistical models are less sensitive to outliers and can provide more accurate\n",
    "estimates of model parameters. For example, robust regression models can be used instead of ordinary least squares \n",
    "regression models.\n",
    "\n",
    "In conclusion, handling outliers is essential to ensure the accuracy and reliability of statistical models and analysis. It\n",
    "is important to carefully evaluate the impact of outliers on the dataset and choose the appropriate technique for handling\n",
    "them based on the specific problem and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c38449-a14e-4db1-baa9-b22d313d1c19",
   "metadata": {},
   "source": [
    "### Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a85d4ab-df2b-40f7-a3da-e12382b39cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b6ac35-c1d9-4c77-95f1-6d60eb54dc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:- There are several techniques that can be used to handle missing data in customer data analysis. Some common techniques\n",
    "are:\n",
    "\n",
    "=> Deleting rows or columns: One approach is to remove the rows or columns with missing data from the analysis. However, \n",
    "this approach can result in a loss of information and reduction in the size of the dataset. Therefore, it is essential to \n",
    "carefully evaluate the impact of deleting rows or columns on the analysis and model performance.\n",
    "\n",
    "=> Imputation: Imputation is a technique that involves replacing missing values with estimated values. There are various \n",
    "methods of imputation, such as mean imputation, median imputation, mode imputation, and regression imputation. Mean \n",
    "imputation involves replacing missing values with the mean of the non-missing values in the same column. Similarly, median \n",
    "imputation involves replacing missing values with the median of the non-missing values in the same column. Mode imputation \n",
    "involves replacing missing values with the mode of the non-missing values in the same column. Regression imputation involves\n",
    "using a regression model to predict the missing values based on the other variables in the dataset.\n",
    "\n",
    "=> Multiple imputation: Multiple imputation is a technique that involves creating multiple imputed datasets and using them \n",
    "for analysis. Multiple imputation provides a more accurate estimate of the missing values and their associated uncertainty.\n",
    "\n",
    "=> K-nearest neighbor imputation: K-nearest neighbor imputation is a technique that involves replacing missing values with\n",
    "values from the k nearest neighbors. The k-nearest neighbors are determined based on the similarity of the other variables\n",
    "in the dataset.\n",
    "\n",
    "In conclusion, handling missing data in customer data analysis is essential to ensure the accuracy and reliability of the \n",
    "analysis and models. It is important to carefully evaluate the impact of missing data on the dataset and choose the \n",
    "appropriate technique for handling them based on the specific problem and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3367577-740b-4695-8e42-5447edfff6b0",
   "metadata": {},
   "source": [
    "### Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca4a8c7-59e1-4b4c-8ebb-b18b5e0ae740",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a926c8-7ea3-4ade-a974-c2716b13a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:- There are several strategies that can be used to determine if the missing data is missing at random (MAR) or if there \n",
    "is a pattern to the missing data:\n",
    "\n",
    "=> Visual inspection: One approach is to create visualizations of the missing data patterns in the dataset. This can be done\n",
    "by plotting the missing data patterns against the other variables in the dataset. If there is a pattern to the missing data, \n",
    "it will be evident in the visualization.\n",
    "\n",
    "=> Statistical tests: Another approach is to use statistical tests to determine if the missing data is missing at random or \n",
    "if there is a pattern to the missing data. This can be done by comparing the characteristics of the data with missing values\n",
    "to the characteristics of the data without missing values. For example, one can use a chi-squared test or t-test to compare \n",
    "the distribution of a variable with missing data to the distribution of the same variable without missing data.\n",
    "\n",
    "=> Imputation: Imputation techniques can also be used to identify patterns in the missing data. This can be done by using\n",
    "different imputation methods to estimate the missing values and comparing the results. If there is a pattern to the missing \n",
    "data, it will be evident in the imputed values.\n",
    "\n",
    "=> Machine learning algorithms: Machine learning algorithms can also be used to identify patterns in the missing data. This \n",
    "can be done by using the missing data as a target variable and training a model to predict the missing values based on the \n",
    "other variables in the dataset. The performance of the model can be used to determine if there is a pattern to the missing \n",
    "data.\n",
    "\n",
    "In conclusion, there are several strategies that can be used to determine if the missing data is missing at random or if \n",
    "there is a pattern to the missing data. It is important to carefully evaluate the characteristics of the data and choose the\n",
    "appropriate strategy based on the specific problem and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691b9ddb-7d1a-4e8d-b658-c30352b20edb",
   "metadata": {},
   "source": [
    "### Q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1195c4-823e-4f38-819d-0188af9b9e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d74de8-0cd2-4638-be56-d0ee467cfad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:- When working with an imbalanced dataset in a medical diagnosis project, it is important to evaluate the performance of\n",
    "the machine learning model carefully. Here are some strategies that can be used to evaluate the performance of the model:\n",
    "\n",
    "\n",
    "=> Confusion matrix: A confusion matrix can be used to evaluate the performance of the machine learning model. The confusion\n",
    "matrix provides information on the number of true positives, true negatives, false positives, and false negatives. From the\n",
    "confusion matrix, various performance metrics such as accuracy, precision, recall, and F1 score can be calculated.\n",
    "\n",
    "=> Resampling techniques: Resampling techniques such as oversampling and undersampling can be used to balance the dataset. \n",
    "Oversampling involves increasing the number of samples in the minority class, while undersampling involves reducing the\n",
    "number of samples in the majority class. Techniques such as SMOTE (Synthetic Minority Over-sampling Technique) can be used\n",
    "to generate synthetic samples in the minority class.\n",
    "\n",
    "=> Cost-sensitive learning: Cost-sensitive learning involves assigning different misclassification costs to the different \n",
    "classes. In a medical diagnosis project, misclassifying a patient with the condition of interest can have more severe \n",
    "consequences than misclassifying a patient without the condition. Therefore, assigning higher misclassification costs to the\n",
    "minority class can improve the performance of the machine learning model.\n",
    "\n",
    "=> Ensemble methods: Ensemble methods such as boosting and bagging can be used to improve the performance of the machine \n",
    "learning model on imbalanced datasets. Boosting involves training multiple weak classifiers and combining them to form a \n",
    "strong classifier, while bagging involves training multiple classifiers on different subsets of the data.\n",
    "\n",
    "In conclusion, evaluating the performance of a machine learning model on an imbalanced dataset in a medical diagnosis \n",
    "project requires careful consideration. Different strategies such as using a confusion matrix, resampling techniques, \n",
    "cost-sensitive learning, and ensemble methods can be used to improve the performance of the machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db173cb-0077-469f-bd9f-5673bd844104",
   "metadata": {},
   "source": [
    "### Q10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b7dc8b-ae48-4fca-a30f-09e566fb499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d86090-be58-4015-82b5-85659cf8e6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:- When dealing with an unbalanced dataset where the majority class is overrepresented, the following methods can be used \n",
    "to balance the dataset and down-sample the majority class:\n",
    "\n",
    "=> Random under-sampling: This method involves randomly selecting a subset of the majority class to match the size of the \n",
    "minority class. This method can result in a loss of information and should be used with caution.\n",
    "\n",
    "=> Cluster-based under-sampling: This method involves clustering the majority class into different groups and then randomly\n",
    "selecting a subset from each cluster. This method can preserve more information than random under-sampling.\n",
    "\n",
    "=> Tomek links: Tomek links are pairs of samples from different classes that are closest to each other. This method involves\n",
    "removing the majority class samples that form Tomek links with minority class samples.\n",
    "\n",
    "=> NearMiss: NearMiss is an under-sampling method that selects samples from the majority class that are closest to the \n",
    "minority class samples.\n",
    "\n",
    "=> Synthetic Minority Over-sampling Technique (SMOTE): SMOTE is an oversampling technique that involves creating synthetic\n",
    "samples in the minority class. The synthetic samples are generated by interpolating between existing samples in the minority\n",
    "class.\n",
    "\n",
    "In conclusion, there are several methods that can be used to balance an unbalanced dataset with the majority class \n",
    "overrepresented. Random under-sampling, cluster-based under-sampling, Tomek links, NearMiss, and SMOTE are some of the \n",
    "popular methods used to down-sample the majority class. It is important to choose the appropriate method based on the\n",
    "specific problem and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36af8104-9043-4902-a843-1cc1150b789b",
   "metadata": {},
   "source": [
    "### Q11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d307a155-e43c-4d1a-bffe-fe0aeac6ae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2934afc4-8c5f-4e81-8c76-b1dad2c46c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:- When dealing with an unbalanced dataset where the minority class is underrepresented, the following methods can be \n",
    "used to balance the dataset and up-sample the minority class:\n",
    "\n",
    "=> Random over-sampling: This method involves randomly replicating samples from the minority class to match the size of the\n",
    "majority class. This method can lead to overfitting and should be used with caution.\n",
    "\n",
    "=> Synthetic Minority Over-sampling Technique (SMOTE): SMOTE is an oversampling technique that involves creating synthetic \n",
    "samples in the minority class. The synthetic samples are generated by interpolating between existing samples in the minority\n",
    "class.\n",
    "\n",
    "=> Adaptive Synthetic Sampling (ADASYN): ADASYN is an adaptive oversampling technique that generates synthetic samples in \n",
    "the minority class based on the density of the minority class samples.\n",
    "\n",
    "=> Synthetic Minority Over-sampling Technique with Kernel Density Estimation (SMOTE-KDE): SMOTE-KDE is an oversampling \n",
    "technique that uses kernel density estimation to generate synthetic samples in the minority class.\n",
    "\n",
    "=> Synthetic Minority Over-sampling Technique with Improved Editing (SMOTE-ENN): SMOTE-ENN is an oversampling technique that\n",
    "combines SMOTE with Edited Nearest Neighbor (ENN) algorithm to remove noisy synthetic samples.\n",
    "\n",
    "In conclusion, when dealing with a dataset with a low percentage of occurrences, it is important to use appropriate methods \n",
    "to balance the dataset and up-sample the minority class. Random over-sampling, SMOTE, ADASYN, SMOTE-KDE, and SMOTE-ENN are\n",
    "some of the popular methods used to up-sample the minority class. It is important to choose the appropriate method based on\n",
    "the specific problem and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abed1597-163c-4c47-88e3-72f23ef860fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
