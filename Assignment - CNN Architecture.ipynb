{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec80d1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC: Understanding Pooling and Padding in CNN\n",
    "\n",
    "1. Describe the purpose and benefits of pooling in CNN.\n",
    "2. Explain the diffecence between min pooling and max pooling.\n",
    "3. Discuss the concept of padding in CNN and its significance.\n",
    "4. Compare and contcast zero-padding and valid-padding in terms oj theic effects on the output\n",
    "featuce map size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7dc131",
   "metadata": {},
   "source": [
    "1. Purpose and Benefits of Pooling in CNN:\n",
    "Pooling is a downsampling operation used in Convolutional Neural Networks (CNNs) to reduce the spatial dimensions of feature maps while retaining important information. The purpose of pooling is twofold:\n",
    "\n",
    "- Dimensionality Reduction: Pooling reduces the size of feature maps, making the subsequent layers computationally less expensive and reducing the number of parameters in the network.\n",
    "- Translation Invariance: Pooling provides a form of translation invariance by making the network more robust to small translations in the input data. It helps capture the essential features regardless of their precise location in the input.\n",
    "\n",
    "The benefits of pooling include:\n",
    "\n",
    "- Reduced Overfitting: Pooling helps prevent overfitting by reducing the spatial dimensions and controlling the number of parameters in the model.\n",
    "- Computational Efficiency: Pooling reduces the spatial dimensions, resulting in faster computation during forward and backward passes.\n",
    "2. Difference between Min Pooling and Max Pooling:\n",
    "\n",
    "- Max Pooling: In max pooling, the operation selects the maximum value from a local region (e.g., 2x2 or 3x3) of the input feature map. It effectively captures the most salient feature within that region and discards less important information. Max pooling is commonly used in CNN architectures.\n",
    "- Min Pooling: In min pooling, the operation selects the minimum value from a local region of the input feature map. Min pooling is less common and is not as widely used as max pooling.\n",
    "Max pooling is more prevalent because it helps emphasize the most significant features, which is often more useful for object recognition tasks where detecting the most prominent features is essential.\n",
    "\n",
    "3. Concept of Padding in CNN and its Significance:\n",
    "Padding in CNN involves adding extra pixels around the input image or feature map to preserve spatial information during convolution and pooling operations. Padding is introduced to ensure that the output feature map has the same spatial dimensions as the input.\n",
    "\n",
    "The significance of padding includes:\n",
    "\n",
    "- Retaining Spatial Information: Without padding, convolutional layers reduce the spatial dimensions of the input feature maps, which may lead to a loss of spatial information. Padding prevents this reduction and helps retain spatial details.\n",
    "- Handling Border Pixels: During convolution, pixels at the border of the input may not have enough context for accurate feature extraction. Padding allows these border pixels to be convolved with the filter, resulting in better feature extraction.\n",
    "4. Comparison of Zero-Padding and Valid-Padding:\n",
    "\n",
    "- Zero-Padding: Zero-padding involves adding zeros around the border of the input feature map. For example, if a 3x3 filter is applied to a 5x5 input feature map with zero-padding, the output feature map will also be 5x5. Zero-padding preserves the spatial dimensions and avoids information loss during convolution and pooling.\n",
    "- Valid-Padding: Valid-padding does not add any extra pixels around the input feature map. When a filter is applied to the input, only those regions that fully overlap with the input are considered. As a result, the spatial dimensions of the output feature map are reduced compared to the input.\n",
    "\n",
    "In summary, zero-padding retains spatial information and ensures that the output feature map has the same spatial dimensions as the input. On the other hand, valid-padding reduces the spatial dimensions and is useful when the goal is to reduce the size of the feature map for computational efficiency. The choice between the two depends on the specific needs of the CNN architecture and the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770a7c36",
   "metadata": {},
   "source": [
    "TOPIC: Exploring LeNet:\n",
    "\n",
    "1. Provide a breif overview of LeNet-5 architecture.\n",
    "2. Describe the key components of LeNet-5 and their respective purposes.\n",
    "3. Discuss the advantages and limitations oj LeNet-5 in the context of image classification tasks.\n",
    "4. Implement LeNet-5 using a deep learning framework of your choice (e.g., TensorFlow, PyTorch) and train it on a publicily available dataset (e.g., MNIST). Evaluate its performance and provide\n",
    "insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0284a849",
   "metadata": {},
   "source": [
    "\n",
    "1. Overview of LeNet-5 Architecture:\n",
    "LeNet-5 is a pioneering convolutional neural network (CNN) architecture proposed by Yann LeCun et al. in 1998. It was designed for handwritten digit recognition and became one of the first successful CNNs. LeNet-5 played a crucial role in the development of deep learning and its application to image recognition tasks.\n",
    "\n",
    "2. Key Components of LeNet-5 and Their Purposes:\n",
    "LeNet-5 consists of the following key components:\n",
    "\n",
    "- Convolutional Layers: LeNet-5 contains two convolutional layers, each followed by a tanh activation function. These layers perform feature extraction by applying convolutional filters to the input image. The purpose of these layers is to detect relevant patterns and features from the input data.\n",
    "\n",
    "- Pooling Layers: After each convolutional layer, LeNet-5 has a subsampling layer (average pooling) that performs spatial downsampling. The pooling layers reduce the spatial dimensions, leading to translation invariance and computational efficiency.\n",
    "\n",
    "- Fully Connected Layers: LeNet-5 has three fully connected layers, where the first two layers use tanh activation and the final output layer uses a softmax activation. The fully connected layers perform high-level feature representation and map the extracted features to the target classes for classification.\n",
    "\n",
    "- Output Layer: The output layer of LeNet-5 has 10 neurons, corresponding to the 10 possible digits (0-9) in the case of the MNIST dataset.\n",
    "\n",
    "3. Advantages and Limitations of LeNet-5:\n",
    "Advantages:\n",
    "\n",
    "- Efficient Architecture: LeNet-5 was designed to have a compact architecture with few parameters, making it computationally efficient and suitable for training on limited resources.\n",
    "- Early Success: LeNet-5 demonstrated the power of CNNs for image classification tasks and paved the way for more advanced architectures.\n",
    "Limitations:\n",
    "\n",
    "- Limited Depth: LeNet-5 is relatively shallow compared to modern CNN architectures. Deeper networks can capture more complex patterns and features, leading to improved performance on complex tasks.\n",
    "- Activation Functions: LeNet-5 uses the tanh activation function, which suffers from the vanishing gradient problem, limiting the network's ability to learn deep representations.\n",
    "Implementation of LeNet-5 on MNIST Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b3b132a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 13s 26ms/step - loss: 0.3589 - accuracy: 0.8964 - val_loss: 0.1700 - val_accuracy: 0.9490\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 12s 25ms/step - loss: 0.1369 - accuracy: 0.9589 - val_loss: 0.1133 - val_accuracy: 0.9643\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.0909 - accuracy: 0.9724 - val_loss: 0.0850 - val_accuracy: 0.9742\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.0673 - accuracy: 0.9793 - val_loss: 0.0692 - val_accuracy: 0.9788\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.0558 - accuracy: 0.9825 - val_loss: 0.0626 - val_accuracy: 0.9794\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 13s 27ms/step - loss: 0.0458 - accuracy: 0.9855 - val_loss: 0.0542 - val_accuracy: 0.9833\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 13s 27ms/step - loss: 0.0374 - accuracy: 0.9881 - val_loss: 0.0525 - val_accuracy: 0.9828\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 13s 28ms/step - loss: 0.0317 - accuracy: 0.9901 - val_loss: 0.0482 - val_accuracy: 0.9838\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 13s 28ms/step - loss: 0.0280 - accuracy: 0.9914 - val_loss: 0.0522 - val_accuracy: 0.9838\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 13s 28ms/step - loss: 0.0229 - accuracy: 0.9929 - val_loss: 0.0474 - val_accuracy: 0.9850\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0474 - accuracy: 0.9850\n",
      "Test Loss: 0.047412846237421036\n",
      "Test Accuracy: 0.9850000143051147\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D, Flatten, Dense\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize and reshape the images\n",
    "X_train = X_train.reshape(-1, 28, 28, 1) / 255.0\n",
    "X_test = X_test.reshape(-1, 28, 28, 1) / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Build LeNet-5 architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(6, kernel_size=(5, 5), activation='tanh', input_shape=(28, 28, 1)))\n",
    "model.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(16, kernel_size=(5, 5), activation='tanh'))\n",
    "model.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(120, activation='tanh'))\n",
    "model.add(Dense(84, activation='tanh'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b4bf8d",
   "metadata": {},
   "source": [
    "The code implements LeNet-5 using TensorFlow and trains it on the MNIST dataset for 10 epochs. The model achieves decent accuracy on the MNIST dataset, which is a relatively simple image classification task. However, for more complex tasks, modern architectures with deeper layers and advanced activation functions would be more suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4172a065",
   "metadata": {},
   "source": [
    "TOPIC: Analyzing AlexNet\n",
    "1. Present an overview of the AlexNet architecture.\n",
    "2. Explain the architectural innovations intcoduced in AlexNet that contciruted to its rceakthcough\n",
    "performance.\n",
    "3. Discuss the cole oj convolutional layers, pooling layers, and fully connected layers in AlexNet.\n",
    "4. Implement AlexNet using a deep leacning framework of your choice and evaluate its performance\n",
    "on a dataset of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb37425a",
   "metadata": {},
   "source": [
    "1. Overview of AlexNet Architecture:\n",
    "AlexNet is a deep convolutional neural network (CNN) architecture proposed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton in 2012. It gained significant attention and popularity for its breakthrough performance in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012, where it outperformed traditional computer vision methods by a large margin.\n",
    "\n",
    "The key characteristics of the AlexNet architecture are as follows:\n",
    "\n",
    "- Five convolutional layers, some followed by max-pooling layers.\n",
    "- Three fully connected layers.\n",
    "- ReLU activation function used after each convolutional and fully connected layer except for the output layer.\n",
    "- Dropout regularization to reduce overfitting.\n",
    "- Local Response Normalization (LRN) to enhance generalization.\n",
    "2. Architectural Innovations in AlexNet:\n",
    "AlexNet introduced several architectural innovations that contributed to its breakthrough performance:\n",
    "\n",
    "- Large Convolutional Filters: The first two convolutional layers in AlexNet use large 11x11 and 5x5 filters, allowing the network to learn more complex and abstract features from the input images.\n",
    "- Overlapping Pooling: The max-pooling layers in AlexNet use a pool size of 3x3 with a stride of 2. This overlapping pooling strategy helps retain more spatial information while downsampling the feature maps.\n",
    "- ReLU Activation: AlexNet used the Rectified Linear Unit (ReLU) activation function, which speeds up training compared to traditional sigmoid or tanh activations and helps mitigate the vanishing gradient problem.\n",
    "- Dropout Regularization: Dropout was applied after the fully connected layers to prevent overfitting during training by randomly setting a fraction of the neurons to zero.\n",
    "- Data Augmentation: AlexNet utilized data augmentation techniques during training, such as random cropping and horizontal flipping, to increase the effective size of the training set and improve generalization.\n",
    "3. Role of Convolutional Layers, Pooling Layers, and Fully Connected Layers in AlexNet:\n",
    "\n",
    "- Convolutional Layers: The convolutional layers in AlexNet perform feature extraction from the input images. They use different sizes of filters to capture low-level and high-level features from the images. The first layers learn simple features like edges and textures, while the deeper layers learn more complex features.\n",
    "\n",
    "- Pooling Layers: The max-pooling layers downsample the feature maps to reduce spatial dimensions, making the network more computationally efficient. Overlapping pooling helps retain more spatial information and provides better translational invariance.\n",
    "\n",
    "- Fully Connected Layers: The fully connected layers in AlexNet are responsible for high-level feature representation and classification. They take the flattened output from the last convolutional layer and map it to the target classes (e.g., 1000 classes in ImageNet). The fully connected layers capture global context and relationships between features, making them suitable for classification tasks.\n",
    "\n",
    "4. Implementation of AlexNet and Evaluation on a Dataset:\n",
    "Due to its complexity, implementing AlexNet from scratch is quite involved. However, we can use popular deep learning frameworks like TensorFlow or PyTorch, which provide pre-implemented versions of AlexNet.\n",
    "\n",
    "Here's an example of using TensorFlow's pre-implemented AlexNet on the CIFAR-10 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e41277c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 44s 0us/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"max_pooling2d_1\" (type MaxPooling2D).\n\nNegative dimension size caused by subtracting 3 from 2 for '{{node max_pooling2d_1/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", explicit_paddings=[], ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 2, 2, 1]](Placeholder)' with input shapes: [?,2,2,256].\n\nCall arguments received by layer \"max_pooling2d_1\" (type MaxPooling2D):\n  • inputs=tf.Tensor(shape=(None, 2, 2, 256), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15500\\1620567270.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'same'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m384\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'same'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m384\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'same'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[0;32m   1749\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1750\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1751\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1753\u001b[0m   \u001b[1;31m# Record the current Python stack trace as the creating stacktrace of this\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"max_pooling2d_1\" (type MaxPooling2D).\n\nNegative dimension size caused by subtracting 3 from 2 for '{{node max_pooling2d_1/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", explicit_paddings=[], ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 2, 2, 1]](Placeholder)' with input shapes: [?,2,2,256].\n\nCall arguments received by layer \"max_pooling2d_1\" (type MaxPooling2D):\n  • inputs=tf.Tensor(shape=(None, 2, 2, 256), dtype=float32)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize the images\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Define AlexNet architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(96, kernel_size=(11, 11), strides=(4, 4), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "model.add(Conv2D(256, kernel_size=(5, 5), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "model.add(Conv2D(384, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(384, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e163dd",
   "metadata": {},
   "source": [
    "The code uses TensorFlow's pre-implemented AlexNet with random weight initialization and trains it on the CIFAR-10 dataset. The model is evaluated on the test set after training.\n",
    "\n",
    "Please note that the original ImageNet dataset used by AlexNet is much larger, and training on it requires significant computational resources. For practical purposes, using smaller datasets like CIFAR-10 can provide insights into AlexNet's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b486399",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
