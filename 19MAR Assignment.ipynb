{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "471ee0bf-e5bb-47c0-8f41-76ee72fa5737",
   "metadata": {},
   "source": [
    "## 19MAR\n",
    "### Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f94161-a7e0-4b18-9834-e37659ae24b3",
   "metadata": {},
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfb01d5-2a80-4237-9acb-4a8ec08c153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f984bea-da4f-4831-9a5f-dcbf0e736465",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:- Min-Max scaling is a data normalization technique used in data preprocessing. It transforms the values of the dataset\n",
    "to a scale between 0 and 1, which helps in improving the performance of machine learning algorithms. The formula for Min-Max \n",
    "scaling is as follows:\n",
    "\n",
    "$x_{scaled} = \\dfrac{x - x_{min}}{x_{max} - x_{min}}$\n",
    "\n",
    "where x is the original value, x_min is the minimum value in the dataset, and x_max is the maximum value in the dataset.\n",
    "\n",
    "For example, suppose we have a dataset of house prices, and we want to normalize the values of the \"price\" column using \n",
    "Min-Max scaling. The original prices range from $100,000 to $1,000,000, with a mean of $500,000. The normalized prices would\n",
    "range from 0 to 1, with a mean of 0.5. Applying Min-Max scaling to this dataset would transform the original prices as \n",
    "follows:\n",
    "\n",
    "Original prices: [100000, 200000, 300000, 400000, 500000, 600000, 700000, 800000, 900000, 1000000]\n",
    "\n",
    "Scaled prices: [0.0, 0.111, 0.222, 0.333, 0.444, 0.556, 0.667, 0.778, 0.889, 1.0]\n",
    "\n",
    "By applying Min-Max scaling to the prices, we can ensure that the prices are on the same scale and that the model can more \n",
    "easily interpret the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21edb3ff-c28e-4a51-8927-606427003c5c",
   "metadata": {},
   "source": [
    "### Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96de3a8f-51d0-4e81-bf50-93335f5b9cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78325199-a741-4d97-bfa9-e77e1f2e94f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:- Unit Vector Scaling is a feature scaling technique that scales the data based on the magnitude of the feature vectors.\n",
    "The technique scales each feature such that their Euclidean norm (magnitude) is 1.\n",
    "\n",
    "The formula for unit vector scaling is:\n",
    "\n",
    "xunit = x/âˆ‘ni=1 (xi)2\n",
    "\n",
    "where $x$ is the original feature vector, $n$ is the number of features in the vector, and $x_{unit}$ is the unit vector\n",
    "scaled feature vector.\n",
    "\n",
    "The main difference between Min-Max scaling and Unit Vector Scaling is that Min-Max scaling scales the values of each \n",
    "feature within a fixed range, while Unit Vector scaling scales the feature values based on the magnitude of the feature \n",
    "vector.\n",
    "\n",
    "An example of Unit Vector Scaling in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeac3669-c83b-4845-a3c3-62fa213d6919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Feature Matrix:\n",
      " [[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "Unit Vector Scaled Feature Matrix:\n",
      " [[0.26726124 0.53452248 0.80178373]\n",
      " [0.45584231 0.56980288 0.68376346]\n",
      " [0.50257071 0.57436653 0.64616234]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# create a feature matrix\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# apply unit vector scaling\n",
    "X_unit = normalize(X, norm='l2')\n",
    "\n",
    "print(\"Original Feature Matrix:\\n\", X)\n",
    "print(\"Unit Vector Scaled Feature Matrix:\\n\", X_unit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52dbbf3-d320-453b-8a41-1d45e5501958",
   "metadata": {},
   "outputs": [],
   "source": [
    "As shown in the output, each feature is scaled based on the magnitude of the feature vector, such that the magnitude of each\n",
    "feature vector in the resulting matrix is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e245d81-2bc5-4054-8812-b31b497c1112",
   "metadata": {},
   "source": [
    "### Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e5c8fd-0330-4b74-89f3-0c230104c2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7714f3af-eeb0-480d-8f70-93ed698f63a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:- PCA (Principal Component Analysis) is a popular technique used for dimensionality reduction. It transforms a\n",
    "high-dimensional dataset into a lower-dimensional dataset by identifying and preserving the most important features that\n",
    "account for the majority of the variance in the original data.\n",
    "\n",
    "The steps for performing PCA are as follows:\n",
    "\n",
    "=> Standardize the data: Subtract the mean from each feature and divide by the standard deviation to ensure that all \n",
    "features are on the same scale.\n",
    "=> Compute the covariance matrix: Calculate the covariance matrix for the standardized data.\n",
    "=> Compute the eigenvectors and eigenvalues of the covariance matrix: The eigenvectors represent the directions of maximum \n",
    "variance in the data, and the eigenvalues represent the magnitude of the variance in each direction.\n",
    "=> Choose the principal components: Select the eigenvectors with the largest eigenvalues, as they account for the most \n",
    "variance in the data.\n",
    "=> Transform the data: Project the original data onto the new lower-dimensional space formed by the selected principal \n",
    "components.\n",
    "Here is an example of how to apply PCA using Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a2f43c-c428-41d8-a534-73098c6e2799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Standardize the data\n",
    "iris_std = (iris.data - iris.data.mean()) / iris.data.std()\n",
    "\n",
    "# Create a PCA object with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit and transform the standardized data using PCA\n",
    "iris_pca = pca.fit_transform(iris_std)\n",
    "\n",
    "# Convert the transformed data to a Pandas DataFrame and plot it\n",
    "iris_df = pd.DataFrame(data=iris_pca, columns=['PC1', 'PC2'])\n",
    "iris_df['target'] = iris.target\n",
    "sns.scatterplot(data=iris_df, x='PC1', y='PC2', hue='target')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3e7d67-bf5d-4cf3-b216-30d32da1effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this example, we load the iris dataset and standardize the data. We then create a PCA object with 2 components and fit \n",
    "and transform the standardized data using PCA. Finally, we convert the transformed data to a Pandas DataFrame and plot it\n",
    "using Seaborn to visualize the results. The resulting plot shows the iris data projected onto the two principal components,\n",
    "with different colors representing the three different iris species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251d7b6f-30d1-4012-ab56-a146a0f23a31",
   "metadata": {},
   "source": [
    "### Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcb3284-4c91-4432-8e3b-d5f8492c8c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110552fe-cffb-44cc-8e21-6f44cb20679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:- PCA can be used as a feature extraction technique to reduce the dimensionality of a dataset by transforming the \n",
    "original features into a smaller set of uncorrelated features called principal components. These principal components\n",
    "represent the directions of maximum variance in the data.\n",
    "\n",
    "PCA works by identifying the eigenvectors and eigenvalues of the covariance matrix of the original data. The eigenvectors \n",
    "represent the directions of maximum variance in the data, and the eigenvalues represent the amount of variance in the data \n",
    "that is accounted for by each eigenvector.\n",
    "\n",
    "To perform feature extraction using PCA, we first standardize the data by subtracting the mean and dividing by the standard \n",
    "deviation. We then compute the covariance matrix and its eigenvectors and eigenvalues. The eigenvectors with the highest\n",
    "eigenvalues represent the principal components, and we can use them as new features in our dataset.\n",
    "\n",
    "For example, let's say we have a dataset with 10 features and we want to reduce it to 3 features using PCA. We can perform \n",
    "PCA on the dataset and select the 3 principal components with the highest eigenvalues. We can then use these 3 principal\n",
    "components as new features in our dataset, effectively reducing the dimensionality of the data.\n",
    "\n",
    "Here's an example code snippet in Python using scikit-learn to perform PCA for feature extraction on the iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31a27493-0e87-4c16-b0af-718719805492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        PC1       PC2       PC3\n",
      "0 -1.359848  0.161815 -0.014142\n",
      "1 -1.375054 -0.089673 -0.106627\n",
      "2 -1.463637 -0.073435  0.009069\n",
      "3 -1.390862 -0.161259  0.015989\n",
      "4 -1.382438  0.165542  0.045636\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Standardize the data\n",
    "iris_std = (iris.data - iris.data.mean()) / iris.data.std()\n",
    "\n",
    "# Create a PCA object with 3 components\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "# Fit and transform the standardized data using PCA\n",
    "iris_pca = pca.fit_transform(iris_std)\n",
    "\n",
    "# Convert the transformed data to a Pandas DataFrame and print the first 5 rows\n",
    "iris_df = pd.DataFrame(data=iris_pca, columns=['PC1', 'PC2', 'PC3'])\n",
    "print(iris_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b886b8f-d8a2-463e-bc20-2d2635dde792",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this example, we first standardize the iris dataset using the mean and standard deviation of the data. We then create a \n",
    "PCA object with 3 components and fit and transform the standardized data using PCA. We finally convert the transformed data \n",
    "to a Pandas DataFrame and print the first 5 rows to see the new features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46308e0f-cceb-4d4e-9413-8aaf2f359ad7",
   "metadata": {},
   "source": [
    "### Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb73477-ffaa-4a89-9d9b-66a8149d1429",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5885ec-e6a5-4528-b5ff-53ced1c38508",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:- In order to use Min-Max scaling to preprocess the data for a recommendation system in a food delivery service, we \n",
    "would first need to identify the features that need to be scaled. In this case, the features are price, rating, and delivery\n",
    "time.\n",
    "\n",
    "Min-Max scaling rescales the features to a range between 0 and 1. To apply Min-Max scaling to the features, we would follow\n",
    "these steps:\n",
    "\n",
    "=> Determine the minimum and maximum values for each feature.\n",
    "=> Subtract the minimum value from each value of the feature.\n",
    "=> Divide each value by the difference between the maximum and minimum values.\n",
    "For example, let's say we have a dataset with the following values for the price, rating, and delivery time features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f30faf7-7d2d-4bc4-9d67-9f789ac2a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "price = [5, 10, 15, 20]\n",
    "rating = [2.5, 3.2, 4.0, 4.5]\n",
    "delivery_time = [20, 30, 45, 60]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831a5e4e-f96b-4e4e-8814-97ba3e39abf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "To apply Min-Max scaling to these features, we would follow these steps:\n",
    "\n",
    "=> Determine the minimum and maximum values for each feature:\n",
    "min_price = 5, max_price = 20\n",
    "min_rating = 2.5, max_rating = 4.5\n",
    "min_delivery_time = 20, max_delivery_time = 60\n",
    "=> Subtract the minimum value from each value of the feature:\n",
    "price_scaled = [(5-5)/(20-5), (10-5)/(20-5), (15-5)/(20-5), (20-5)/(20-5)] = [0.0, 0.333, 0.667, 1.0]\n",
    "rating_scaled = [(2.5-2.5)/(4.5-2.5), (3.2-2.5)/(4.5-2.5), (4.0-2.5)/(4.5-2.5), (4.5-2.5)/(4.5-2.5)] = [0.0, 0.4, 0.8, 1.0]\n",
    "delivery_time_scaled = [(20-20)/(60-20), (30-20)/(60-20), (45-20)/(60-20), (60-20)/(60-20)] = [0.0, 0.333, 0.778, 1.0]\n",
    "=> The scaled features can now be used for analysis.\n",
    "By applying Min-Max scaling, we have rescaled the features to a range between 0 and 1, which can help to prevent features \n",
    "with larger numerical ranges from dominating the analysis. This can lead to more accurate results and better performance in\n",
    "the recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acc4fd3-8d76-4a08-adae-d1d4fa6804cb",
   "metadata": {},
   "source": [
    "### Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5906ed-38a5-4aa4-a4db-7645a2bbcfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60039d4e-059b-4cae-adb7-ed7ac446339d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:- PCA (Principal Component Analysis) is a popular technique used for dimensionality reduction in machine learning. It \n",
    "aims to reduce the number of features in a dataset while retaining as much of the original information as possible. In the \n",
    "context of a stock price prediction project, PCA can be used to identify the most important variables that explain the \n",
    "variation in the stock prices.\n",
    "\n",
    "To use PCA for dimensionality reduction, you would follow these steps:\n",
    "\n",
    "=> Standardize the data: It is important to standardize the data so that each feature has a mean of zero and a standard \n",
    "deviation of one. This is necessary because PCA is a variance-based method, and the variance of the features needs to be \n",
    "comparable.\n",
    "\n",
    "=> Compute the covariance matrix: The covariance matrix describes the relationship between the features in the dataset. It \n",
    "can be computed by taking the dot product of the transpose of the standardized data and the standardized data itself.\n",
    "\n",
    "=> Compute the eigenvalues and eigenvectors of the covariance matrix: The eigenvectors of the covariance matrix represent \n",
    "the principal components of the dataset, while the eigenvalues represent the amount of variance explained by each principal\n",
    "component.\n",
    "\n",
    "=> Choose the number of principal components: You can choose the number of principal components to retain based on the \n",
    "amount of variance explained by each component. Typically, you would choose the smallest number of components that explain a \n",
    "significant amount of the variance in the data.\n",
    "\n",
    "=> Project the data onto the new feature space: Finally, you can use the eigenvectors corresponding to the selected \n",
    "principal components to project the data onto the new feature space.\n",
    "\n",
    "In the context of a stock price prediction project, you might use PCA to identify the most important variables that explain\n",
    "the variation in the stock prices. For example, you might start with a dataset that contains features such as company \n",
    "financial data (e.g., revenue, earnings, and assets) and market trends (e.g., interest rates, inflation, and GDP growth).\n",
    "After standardizing the data, you would compute the covariance matrix and the eigenvalues and eigenvectors of the matrix. \n",
    "You might choose to retain the top three principal components, which together explain 80% of the variance in the data. You\n",
    "would then project the data onto the new feature space defined by these principal components, and use these new features as \n",
    "inputs to your stock price prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b63858-a440-4bbf-8d11-1869883dff60",
   "metadata": {},
   "source": [
    "### Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ae2a83-e058-4397-a1a8-474706148f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40dfb4e-3fb1-46a8-8729-135486d78cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:- To perform Min-Max scaling, we need to apply the following formula:\n",
    "\n",
    "X_norm = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "where X_min and X_max are the minimum and maximum values in the dataset, respectively.\n",
    "\n",
    "In this case, X_min = 1 and X_max = 20. Therefore, we have:\n",
    "\n",
    "First, we calculate the numerator for each value in the dataset:\n",
    "\n",
    "For 1: (1 - 1) = 0\n",
    "For 5: (5 - 1) = 4\n",
    "For 10: (10 - 1) = 9\n",
    "For 15: (15 - 1) = 14\n",
    "For 20: (20 - 1) = 19\n",
    "Then, we calculate the denominator for each value in the dataset:\n",
    "\n",
    "For 1: (20 - 1) = 19\n",
    "For 5: (20 - 1) = 19\n",
    "For 10: (20 - 1) = 19\n",
    "For 15: (20 - 1) = 19\n",
    "For 20: (20 - 1) = 19\n",
    "Finally, we calculate the normalized value for each value in the dataset:\n",
    "\n",
    "For 1: 0/19 = 0\n",
    "For 5: 4/19 = 0.21\n",
    "For 10: 9/19 = 0.47\n",
    "For 15: 14/19 = 0.74\n",
    "For 20: 19/19 = 1\n",
    "Therefore, the Min-Max scaled dataset is [-1, -0.37, 0.05, 0.47, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8129bdd1-13fb-4b6c-b00d-b7264666991d",
   "metadata": {},
   "source": [
    "### Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004b1e55-a0dd-49e6-8abd-cf80274b8ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8260bb-18fb-4ebd-bd48-76711c56b217",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:- The number of principal components to retain depends on the amount of variance explained by each component. To \n",
    "determine the number of components to retain, we can look at the explained variance ratio (EVR), which tells us the \n",
    "proportion of the total variance in the data that is explained by each principal component.\n",
    "\n",
    "To perform PCA on the given dataset, we first need to standardize the features to have zero mean and unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6442f4fa-f09c-4588-81e8-d59d22ca1405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Fit PCA and transform the data\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Look at the explained variance ratio\n",
    "print(pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f0ed0f-e34b-4cd1-8bff-fb10b4c5dc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "The output will show the proportion of variance explained by each principal component. We can then decide how many \n",
    "components to retain based on a certain threshold. For example, we might choose to retain enough components to explain\n",
    "90% of the variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1320c7b2-abea-4671-9c53-5eb6db188acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1.]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Look at the cumulative explained variance ratio\n",
    "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "print(cumulative_variance_ratio)\n",
    "\n",
    "# Find the number of components needed to explain 90% of the variance\n",
    "n_components = np.argmax(cumulative_variance_ratio >= 0.9) + 1\n",
    "print(n_components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0be62b8-3c03-4c96-8547-915a8fb71343",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this case, the output might show that the first two components explain 70% of the variance, while the third component \n",
    "explains 20%, and the remaining two components explain only 10% together. Therefore, we might choose to retain the first two\n",
    "principal components, which capture most of the variance in the data. We would then use these two components as new features\n",
    "in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815cfd31-4755-4ff4-a6d8-5e43712a9c41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
