{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db5cd805",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3ff5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f95e296",
   "metadata": {},
   "source": [
    "Ans:-\n",
    "    KNN stands for \"k-nearest neighbors,\" and it is a simple and popular machine learning algorithm used for both classification and regression tasks. The KNN algorithm is a type of instance-based learning, where it makes predictions for new data points by finding the \"k\" nearest data points (neighbors) in the training set and using their labels (in the case of classification) or their average value (in the case of regression) to make the prediction.\n",
    "\n",
    "Here's how the KNN algorithm works:\n",
    "\n",
    "1. Data Preparation: The KNN algorithm requires a labeled dataset to train on. The dataset consists of samples with their corresponding labels (for classification) or numerical target values (for regression).\n",
    "\n",
    "2. Distance Metric: To find the nearest neighbors, a distance metric (e.g., Euclidean distance, Manhattan distance, etc.) is used to measure the similarity or dissimilarity between data points in the feature space.\n",
    "\n",
    "3. Choosing \"k\": The parameter \"k\" is a hyperparameter that needs to be set before training the model. It represents the number of nearest neighbors to consider when making predictions for a new data point. A larger \"k\" value tends to smooth the decision boundaries in classification and reduce the effect of noise, while a smaller \"k\" value can lead to a more complex decision boundary.\n",
    "\n",
    "4. Prediction (Classification): For a new data point, the algorithm identifies the \"k\" nearest neighbors based on the chosen distance metric. It then looks at the labels of these \"k\" neighbors and assigns the most common label among them to the new data point.\n",
    "\n",
    "5. Prediction (Regression): For regression tasks, the KNN algorithm takes the average value of the target variables of the \"k\" nearest neighbors as the predicted value for the new data point.\n",
    "\n",
    "6. Model Evaluation: The performance of the KNN algorithm can be evaluated using various metrics like accuracy (for classification), mean squared error (for regression), or other suitable evaluation metrics.\n",
    "\n",
    "7. Hyperparameter Tuning: The value of \"k\" and the choice of distance metric are hyperparameters that need to be tuned during model training. This is usually done using techniques like cross-validation to find the best values for these hyperparameters.\n",
    "\n",
    "KNN is a non-parametric algorithm, which means it does not make any assumptions about the underlying data distribution. It is relatively simple to implement and understand, making it a good choice for baseline models. However, it can be computationally expensive for large datasets, as it requires computing distances between the new data point and all the points in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae463e",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44b1ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d778bddf",
   "metadata": {},
   "source": [
    "Ans:-\n",
    "    \n",
    "    Choosing the value of \"k\" in KNN (k-nearest neighbors) is an important hyperparameter tuning step, as it significantly impacts the performance of the algorithm. The choice of \"k\" can affect the model's bias, variance, and generalization ability. Here are some common approaches to choose the value of \"k\" in KNN:\n",
    "\n",
    "1. Cross-Validation: One of the most common methods to select the value of \"k\" is through cross-validation. The dataset is split into training and validation sets, and the KNN algorithm is trained and evaluated for different values of \"k\" on the validation set. The value of \"k\" that results in the best performance (e.g., highest accuracy for classification or lowest mean squared error for regression) on the validation set is chosen as the optimal \"k.\"\n",
    "\n",
    "2. Elbow Method: The elbow method is a graphical approach used for selecting the value of \"k.\" It involves plotting the performance metric (e.g., accuracy or mean squared error) against different values of \"k.\" The plot typically shows a decreasing trend in performance as \"k\" increases (overfitting) and a stabilizing trend after a certain point. The \"k\" value where the performance stabilizes or starts to degrade is often considered as the optimal \"k.\"\n",
    "\n",
    "3. Domain Knowledge or Problem-Specific Considerations: Sometimes, domain knowledge or problem-specific considerations can guide the choice of \"k.\" For example, in cases where the data has a natural grouping structure, the value of \"k\" might be set to the number of groups to ensure that each group has a representative number of neighbors.\n",
    "\n",
    "4. Odd vs. Even \"k\": In binary classification problems, using an odd value of \"k\" is preferred to avoid ties in class votes. An odd \"k\" ensures that the majority class is unambiguous, leading to a definite prediction. In regression tasks, using an even \"k\" can be useful to avoid ties when computing the average value of the target variables.\n",
    "\n",
    "5. Grid Search: In some cases, an exhaustive grid search over a predefined range of \"k\" values can be performed to find the best value based on a specified evaluation metric.\n",
    "\n",
    "6. Rule of Thumb: As a rule of thumb, the value of \"k\" is often chosen to be a small odd integer (e.g., 3, 5, 7). Smaller values of \"k\" can capture local patterns but may be sensitive to noise, while larger values of \"k\" smooth the decision boundary but may miss local patterns.\n",
    "\n",
    "It is essential to remember that the choice of \"k\" is problem-specific, and what works best for one dataset may not necessarily be optimal for another. Hence, it is crucial to experiment with different values of \"k\" and select the one that results in the best model performance on the validation set or through cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff234916",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e53511",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27689917",
   "metadata": {},
   "source": [
    "Ans:-\n",
    "    The main difference between KNN classifier and KNN regressor lies in their application and the type of output they produce:\n",
    "\n",
    "1. KNN Classifier:\n",
    "\n",
    "- KNN classifier is used for classification tasks, where the goal is to predict the class or category of a new data point based on its feature values.\n",
    "- The output of the KNN classifier is a categorical label, representing the predicted class to which the new data point belongs.\n",
    "- The algorithm finds the \"k\" nearest neighbors of the new data point in the training set and assigns the most common class among these neighbors as the predicted class for the new data point.\n",
    "- The predicted class is often determined by a majority vote among the \"k\" neighbors, with each neighbor's vote being weighted equally.\n",
    "- The performance of the KNN classifier is usually evaluated using metrics such as accuracy, precision, recall, F1-score, etc.\n",
    "2. KNN Regressor:\n",
    "\n",
    "- KNN regressor is used for regression tasks, where the goal is to predict a continuous numerical value for a new data point based on its feature values.\n",
    "- The output of the KNN regressor is a numerical value, representing the predicted target value for the new data point.\n",
    "- The algorithm finds the \"k\" nearest neighbors of the new data point in the training set and takes the average (or weighted average) of the target values of these neighbors as the predicted target value for the new data point.\n",
    "- The predicted target value is often determined by the average of the \"k\" neighbors' target values, with each neighbor's contribution being weighted based on their proximity to the new data point.\n",
    "- The performance of the KNN regressor is usually evaluated using metrics such as mean squared error (MSE), mean absolute error (MAE), R-squared, etc.\n",
    "\n",
    "\n",
    "In summary, KNN classifier is used for classification tasks, and it predicts the class label for a new data point, whereas KNN regressor is used for regression tasks and predicts a numerical value for a new data point. The key distinction is in the type of output they produce, with the classifier providing discrete class labels, and the regressor providing continuous numerical predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d86aab",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40ad7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c29722",
   "metadata": {},
   "source": [
    "Ans:-\n",
    "    \n",
    "    The performance of the KNN algorithm can be measured using various evaluation metrics, which depend on the specific task being performed (classification or regression). Here are some commonly used performance metrics for evaluating the KNN algorithm:\n",
    "\n",
    "#### Classification Metrics:\n",
    "1. Accuracy: Accuracy is the most common metric for classification tasks. It measures the proportion of correctly classified data points out of the total number of data points in the test set. Higher accuracy values indicate better performance.\n",
    "\n",
    "2. Precision: Precision measures the proportion of true positive predictions (correctly predicted positive instances) out of all positive predictions. It is particularly useful when the cost of false positives is high.\n",
    "\n",
    "3. Recall (Sensitivity or True Positive Rate): Recall measures the proportion of true positive predictions out of all actual positive instances in the test set. It is particularly useful when the cost of false negatives is high.\n",
    "\n",
    "4. F1-Score: The F1-score is the harmonic mean of precision and recall. It provides a balanced measure of both precision and recall and is suitable when the data is imbalanced.\n",
    "\n",
    "5. Confusion Matrix: The confusion matrix provides a comprehensive summary of the classification results. It shows the true positive, true negative, false positive, and false negative predictions.\n",
    "\n",
    "#### Regression Metrics:\n",
    "\n",
    "1. Mean Squared Error (MSE): MSE measures the average squared difference between the predicted and true target values. Lower MSE values indicate better performance.\n",
    "\n",
    "2. Mean Absolute Error (MAE): MAE measures the average absolute difference between the predicted and true target values. It is less sensitive to outliers compared to MSE.\n",
    "\n",
    "3. R-squared (Coefficient of Determination): R-squared measures the proportion of variance in the target variable that is explained by the model. Higher R-squared values indicate better model fit.\n",
    "\n",
    "4. Root Mean Squared Error (RMSE): RMSE is the square root of MSE and provides a measure of the model's prediction error in the original unit of the target variable.\n",
    "\n",
    "5. Mean Absolute Percentage Error (MAPE): MAPE measures the average percentage difference between the predicted and true target values. It is often used when the scale of the target variable is significant.\n",
    "\n",
    "When evaluating the performance of the KNN algorithm, it is essential to consider the specific problem context and select the appropriate evaluation metric that aligns with the task's requirements and objectives. Cross-validation is also commonly used to assess the model's performance on different data splits and obtain more robust performance estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3ba82f",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01022a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f63e3b",
   "metadata": {},
   "source": [
    "Ans:-\n",
    "    \n",
    "    The curse of dimensionality in KNN refers to the phenomenon where the performance of the K-nearest neighbors algorithm degrades as the number of dimensions (features) in the dataset increases. In high-dimensional spaces, KNN becomes less effective and less efficient due to certain challenges that arise:\n",
    "\n",
    "1. Increased Sparsity: As the number of dimensions increases, the data becomes more sparse. In a high-dimensional space, the volume of the space increases exponentially, leading to a situation where the available data points are sparsely distributed. As a result, finding the nearest neighbors becomes more difficult, and the neighbors may be located far away from the query point.\n",
    "\n",
    "2. Curse of Distance: In high-dimensional spaces, the concept of distance loses its meaning. The distance between points tends to become similar, making it challenging to distinguish between close and far neighbors. Consequently, the notion of \"nearness\" becomes less meaningful, and KNN may not accurately capture local patterns in the data.\n",
    "\n",
    "3. Increased Computational Complexity: As the number of dimensions increases, the computational complexity of the KNN algorithm grows exponentially. Calculating distances between data points in high-dimensional spaces becomes computationally expensive and time-consuming, especially for large datasets.\n",
    "\n",
    "4. Data Sparsity and Overfitting: The risk of overfitting increases with the curse of dimensionality. In high-dimensional spaces, even a moderate-sized dataset may appear large due to the exponential growth in volume. Consequently, the risk of the model memorizing the training data increases, leading to poorer generalization to unseen data.\n",
    "\n",
    "5. Increased Number of Samples Required: As the dimensionality increases, the number of samples required to adequately represent the data distribution also increases. In high-dimensional spaces, obtaining a sufficient number of samples to cover the entire feature space becomes impractical, leading to a risk of overfitting or inadequate representation of the data.\n",
    "\n",
    "To mitigate the curse of dimensionality in KNN and other high-dimensional data problems, dimensionality reduction techniques (e.g., Principal Component Analysis, t-SNE, etc.) can be applied to transform the data into a lower-dimensional space that retains most of the essential information while reducing the computational burden. Additionally, feature selection and extraction methods can be used to retain only the most relevant features for the specific task, reducing the dimensionality and potentially improving the performance of KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f045da12",
   "metadata": {},
   "source": [
    "# Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae6f742",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f679be26",
   "metadata": {},
   "source": [
    "Ans:-\n",
    "    \n",
    "    Handling missing values in the K-nearest neighbors (KNN) algorithm requires special consideration, as KNN is a distance-based algorithm that relies on the similarity between data points. When dealing with missing values, there are several approaches to address the issue:\n",
    "\n",
    "1. Dropping Rows: One simple approach is to remove rows (data points) that contain missing values. However, this method can lead to data loss and may not be practical when a large portion of the data contains missing values.\n",
    "\n",
    "2. Imputation with Mean/Median/Mode: Missing values can be replaced with the mean, median, or mode of the feature (column) they belong to. This method can work well for numerical features and may help preserve the general distribution of the data. However, it may not be suitable for categorical features.\n",
    "\n",
    "3. Imputation with Constant Value: Another option is to replace missing values with a constant value, such as zero or a unique identifier, to indicate that the value is missing. This approach allows you to retain the original data but may not be appropriate for all situations.\n",
    "\n",
    "4. Imputation with KNN: For missing values in a specific feature, you can use KNN to impute the missing values based on the values of the nearest neighbors. You can treat each feature with missing values as a separate problem and use KNN to find the \"k\" nearest neighbors for each missing value and impute it with the average (for numerical) or mode (for categorical) value of those neighbors.\n",
    "\n",
    "5. Iterative Imputation: In some cases, an iterative imputation approach can be used. This involves iteratively imputing missing values based on the values imputed in previous iterations until convergence.\n",
    "\n",
    "6. Using Distance Metrics with Missing Values: Some distance metrics, such as the Manhattan distance (L1 distance), can handle missing values by treating them as zero. However, the effectiveness of this approach depends on the nature of the data and the distance metric used.\n",
    "\n",
    "It is essential to consider the nature of the data, the amount of missing data, and the impact of different imputation methods on the results when handling missing values in KNN. Additionally, preprocessing techniques, such as data normalization or standardization, can be applied before imputing missing values to ensure that the imputation process is not biased by the scale of the features. As always, the choice of the imputation method should be based on domain knowledge, data characteristics, and the specific requirements of the analysis or modeling task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30be5683",
   "metadata": {},
   "source": [
    "# Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1217c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336926a1",
   "metadata": {},
   "source": [
    "Ans:-\n",
    "    \n",
    "    The performance of the KNN classifier and KNN regressor can vary depending on the nature of the problem and the characteristics of the data. Here is a comparison and contrast between the two:\n",
    "\n",
    "### KNN Classifier:\n",
    "\n",
    "- Task: KNN classifier is used for classification tasks, where the goal is to predict the class or category of a new data point based on its feature values.\n",
    "- Output: The output of the KNN classifier is a categorical label, representing the predicted class to which the new data point belongs.\n",
    "- Evaluation Metrics: Common evaluation metrics for KNN classifier include accuracy, precision, recall, F1-score, and confusion matrix.\n",
    "- Pros: KNN classifier is simple to understand and implement. It can handle multi-class classification problems and can capture complex decision boundaries. It is also robust to outliers.\n",
    "- Cons: The performance of KNN classifier can suffer in high-dimensional spaces (curse of dimensionality) and with imbalanced datasets. It can be computationally expensive for large datasets as it requires computing distances between data points.\n",
    "### KNN Regressor:\n",
    "\n",
    "- Task: KNN regressor is used for regression tasks, where the goal is to predict a continuous numerical value for a new data point based on its feature values.\n",
    "- Output: The output of the KNN regressor is a numerical value, representing the predicted target value for the new data point.\n",
    "- Evaluation Metrics: Common evaluation metrics for KNN regressor include mean squared error (MSE), mean absolute error (MAE), R-squared, and root mean squared error (RMSE).\n",
    "- Pros: KNN regressor is easy to implement and can handle both single-output and multi-output regression problems. It is capable of capturing non-linear relationships and is robust to outliers.\n",
    "- Cons: Similar to the KNN classifier, KNN regressor can suffer from the curse of dimensionality in high-dimensional spaces. It may not perform well on datasets with a large number of missing values.\n",
    "#### Which one is better for which type of problem?\n",
    "\n",
    "- For Classification Problems: Use KNN classifier when dealing with classification tasks, especially when the decision boundaries are complex and not easily represented by linear models. KNN classifier can be effective when the data is not too high-dimensional and when the dataset is not heavily imbalanced. However, it may require tuning of the \"k\" value and careful handling of imbalanced datasets.\n",
    "- For Regression Problems: Use KNN regressor when predicting continuous numerical values is the primary objective. KNN regressor can be useful when there is a non-linear relationship between features and the target variable. However, it is essential to be cautious of the curse of dimensionality in high-dimensional spaces, as the performance of KNN regressor can degrade with increased dimensions.\n",
    "\n",
    "\n",
    "In practice, it is recommended to try both KNN classifier and KNN regressor and compare their performances on a validation or test set to choose the one that performs better for the specific problem. Additionally, pre-processing techniques like feature scaling and dimensionality reduction can be employed to improve the performance of both KNN classifier and regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3e88ab",
   "metadata": {},
   "source": [
    "# Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35365a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52258554",
   "metadata": {},
   "source": [
    "Ans:-\n",
    "    \n",
    "    Strengths of KNN Algorithm:\n",
    "\n",
    "1. Simplicity: KNN is a simple and easy-to-understand algorithm, making it accessible to beginners and non-experts in machine learning.\n",
    "\n",
    "2. Non-parametric: KNN is a non-parametric algorithm, which means it makes no assumptions about the underlying data distribution. This makes it more flexible and applicable to various types of data.\n",
    "\n",
    "3. Handles Non-linear Data: KNN can handle non-linear relationships between features and the target variable, allowing it to capture complex patterns in the data.\n",
    "\n",
    "4. No Training Phase: KNN is an instance-based learning algorithm, meaning it does not have an explicit training phase. It only requires storing the training data, making it computationally efficient during training.\n",
    "\n",
    "5. Robust to Outliers: KNN is less sensitive to outliers compared to some other algorithms like linear regression or SVM. Outliers have a limited impact on the final predictions, as they are averaged out by the contributions of multiple neighbors.\n",
    "\n",
    "Weaknesses of KNN Algorithm:\n",
    "\n",
    "1. Computational Complexity: KNN's computational complexity increases with the size of the training dataset, as it requires calculating distances between the new data point and all the data points in the training set. This can make it slow and memory-intensive for large datasets.\n",
    "\n",
    "2. Curse of Dimensionality: KNN performance suffers in high-dimensional feature spaces due to increased sparsity, the curse of distance, and computational inefficiency.\n",
    "\n",
    "3. Imbalanced Data: In classification tasks, KNN can be biased toward the majority class in imbalanced datasets, as it tends to have more neighbors from the majority class.\n",
    "\n",
    "4. Sensitivity to Parameter \"k\": The choice of the \"k\" value can significantly impact the performance of the KNN algorithm. A small \"k\" can lead to noise sensitivity, while a large \"k\" can result in oversmoothing.\n",
    "\n",
    "Addressing Weaknesses:\n",
    "\n",
    "1. Dimensionality Reduction: To address the curse of dimensionality, perform dimensionality reduction techniques like Principal Component Analysis (PCA) or feature selection to reduce the number of dimensions and retain the most informative features.\n",
    "\n",
    "2. Distance Metric Optimization: Experiment with different distance metrics to find the one that best fits the data distribution and problem domain. Weighted distance metrics or custom distance functions can also be used to give more importance to certain features.\n",
    "\n",
    "3. k Selection through Cross-Validation: Use cross-validation to find the optimal value of \"k\" that provides the best performance on validation data.\n",
    "\n",
    "4. Data Preprocessing: Standardize or normalize the features to ensure that all features have equal influence on the distance calculation.\n",
    "\n",
    "5. Handling Imbalanced Data: For imbalanced datasets, consider using techniques like oversampling, undersampling, or using weighted KNN to balance the class distribution.\n",
    "\n",
    "6. Approximate Nearest Neighbor (ANN) Methods: For large datasets, consider using Approximate Nearest Neighbor (ANN) methods or data structures like KD-trees to speed up the search for nearest neighbors.\n",
    "\n",
    "By understanding the strengths and weaknesses of the KNN algorithm and employing appropriate strategies, the limitations of KNN can be mitigated, making it a powerful and versatile tool for both classification and regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ade7ef",
   "metadata": {},
   "source": [
    "# Q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ed5494",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c03cb1",
   "metadata": {},
   "source": [
    "Ans:-\n",
    "    \n",
    "    \n",
    "Euclidean distance and Manhattan distance are two commonly used distance metrics in the K-nearest neighbors (KNN) algorithm. They both measure the distance between data points in a feature space and are used to determine the similarity or dissimilarity between data points. Here are the key differences between Euclidean distance and Manhattan distance:\n",
    "\n",
    "### Euclidean Distance:\n",
    "\n",
    "- Euclidean distance is also known as the straight-line distance or L2 norm.\n",
    "- It is the most commonly used distance metric in KNN.\n",
    "- In a 2-dimensional space, Euclidean distance between two points (x1, y1) and (x2, y2) is given by:\n",
    "- Euclidean_distance = sqrt((x2 - x1)^2 + (y2 - y1)^2)\n",
    "- In a multidimensional space, Euclidean distance between two points (x1, x2, ..., xn) and (y1, y2, ..., yn) is given by:\n",
    "- Euclidean_distance = sqrt((x1 - y1)^2 + (x2 - y2)^2 + ... + (xn - yn)^2)\n",
    "- Euclidean distance considers the actual geometric distance between two points and takes into account the magnitude of differences in all dimensions.\n",
    "- It gives higher weight to large differences between feature values and is sensitive to the scale of the features.\n",
    "### Manhattan Distance:\n",
    "  \n",
    "- Manhattan distance is also known as the city block distance, L1 norm, or taxicab distance.\n",
    "- In a 2-dimensional space, Manhattan distance between two points (x1, y1) and (x2, y2) is given by:\n",
    "- Manhattan_distance = |x2 - x1| + |y2 - y1|\n",
    "- In a multidimensional space, Manhattan distance between two points (x1, x2, ..., xn) and (y1, y2, ..., yn) is given by:\n",
    "- Manhattan_distance = |x1 - y1| + |x2 - y2| + ... + |xn - yn|\n",
    "- Manhattan distance measures the distance in terms of the sum of absolute differences between the feature values in each dimension.\n",
    "- It computes the distance by following the grid-like path a taxi would take in a city with streets aligned in a grid-like pattern.\n",
    " -Manhattan distance gives equal weight to differences along each dimension and is less sensitive to the scale of the features.\n",
    "### Comparison:\n",
    "\n",
    "- Euclidean distance calculates the shortest distance between two points in a straight line, while Manhattan distance calculates the distance by following axis-aligned paths (horizontal and vertical).\n",
    "- Euclidean distance is more sensitive to differences in all dimensions, while Manhattan distance is less sensitive and gives equal importance to each dimension's difference.\n",
    "- In KNN, Euclidean distance is generally preferred when the feature space is continuous and the scale of features is meaningful. On the other hand, Manhattan distance is more suitable for data with nominal or ordinal features or when dealing with high-dimensional data. Additionally, Manhattan distance is often preferred in cases where there are outliers or when the features have very different scales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1e21bb",
   "metadata": {},
   "source": [
    "# Q10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3728ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4406cb1b",
   "metadata": {},
   "source": [
    "Ans:-\n",
    "    \n",
    "    The role of feature scaling in K-nearest neighbors (KNN) is to ensure that all features contribute equally to the distance calculations between data points. Since KNN relies on measuring distances to find the nearest neighbors, the scale of the features can significantly impact the algorithm's performance. Feature scaling is essential in KNN for the following reasons:\n",
    "\n",
    "1. Equalizing Feature Influence: Features with larger scales can dominate the distance calculations, overshadowing features with smaller scales. As a result, KNN may primarily rely on the dominant features, leading to suboptimal results. Feature scaling ensures that all features have equal weight in the distance calculations, preventing any particular feature from dominating the decision process.\n",
    "\n",
    "2. Distance Metric Consistency: KNN uses distance metrics (e.g., Euclidean distance or Manhattan distance) to measure the similarity between data points. For accurate distance calculations, it is crucial to have features with similar scales. When features have different scales, the distance metric may be biased towards the features with larger scales, leading to incorrect similarities between data points.\n",
    "\n",
    "3. Curse of Dimensionality: In high-dimensional spaces, the distance between data points can become less meaningful, and the curse of dimensionality can exacerbate the issue when features have different scales. Feature scaling can mitigate the effects of the curse of dimensionality by normalizing the distances in the feature space.\n",
    "\n",
    "Common methods of feature scaling used in KNN include:\n",
    "\n",
    "1. Min-Max Scaling (Normalization): This method scales the features to a fixed range, usually [0, 1]. It is achieved by subtracting the minimum value and dividing by the range (max-min) of each feature.\n",
    "\n",
    "2. Standardization (Z-score Scaling): This method scales the features to have zero mean and unit variance. It involves subtracting the mean and dividing by the standard deviation of each feature.\n",
    "\n",
    "3. Robust Scaling: This method is similar to standardization but uses the median and interquartile range (IQR) instead of the mean and standard deviation. It is more robust to outliers.\n",
    "\n",
    "4. Log Transformation: For features with heavily skewed distributions, log transformation can be applied to make the distribution more symmetric and improve scaling.\n",
    "\n",
    "By applying feature scaling before using the KNN algorithm, we ensure that the distance calculations are more meaningful and that all features contribute equally to the prediction process. Proper feature scaling can lead to improved model performance, better convergence, and more reliable results in KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daae4662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8b61df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
