{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca162fd0-f56c-4eb3-a1e7-f5aa2ead607e",
   "metadata": {},
   "source": [
    "## 26MAR\n",
    "### Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e180d26-8f33-4147-a94f-0eef77b26b01",
   "metadata": {},
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c40a31-8099-493c-8f77-003c0cb0f351",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35324b57-ec1c-4963-98cd-bac88f86db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:- Linear regression is a statistical method used to establish a relationship between a dependent variable and \n",
    "one or more independent variables. It is commonly used in data analysis and prediction.\n",
    "\n",
    "Simple linear regression involves finding a linear relationship between two variables. It uses one independent \n",
    "variable to predict the dependent variable. For instance, suppose we want to determine the relationship between a \n",
    "student's test score and the number of hours they studied. We can plot the data on a scatter plot, where the number \n",
    "of hours studied is the independent variable, and the test score is the dependent variable. We can then fit a line\n",
    "to the data and use it to predict the test score for any number of hours studied. This is a simple linear \n",
    "regression.\n",
    "\n",
    "Multiple linear regression, on the other hand, involves finding a linear relationship between a dependent variable \n",
    "and two or more independent variables. It is used when there is more than one variable that may impact the \n",
    "dependent variable. For instance, suppose we want to determine the factors that affect a student's GPA. We can use\n",
    "multiple linear regression by considering the student's study hours, attendance, and family income as independent \n",
    "variables. We can then fit a line to the data and use it to predict the student's GPA based on these factors.\n",
    "\n",
    "To summarize, simple linear regression uses one independent variable to predict the dependent variable, while \n",
    "multiple linear regression uses two or more independent variables to predict the dependent variable.\n",
    "\n",
    "Example of Simple Linear Regression: Suppose we want to determine the relationship between a person's height and \n",
    "their weight. We can plot the data on a scatter plot, where the height is the independent variable, and the weight\n",
    "is the dependent variable. We can then fit a line to the data and use it to predict the weight of a person based \n",
    "on their height.\n",
    "\n",
    "Example of Multiple Linear Regression: Suppose we want to determine the factors that impact the price of a house.\n",
    "We can use multiple linear regression by considering the house size, the number of bedrooms, the location, and the \n",
    "age of the house as independent variables. We can then fit a line to the data and use it to predict the house price\n",
    "based on these factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c12d65c-2805-4d67-8318-53067059e2fe",
   "metadata": {},
   "source": [
    "### Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d31b9ff-c46c-427f-b66e-ac4531ecceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e82cfa8-3eda-429d-8e57-95e0688bb339",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:- Linear regression is a statistical method used to establish a relationship between a dependent variable and one or more independent variables. However, the validity of the results obtained from linear regression analysis depends on certain assumptions about the data. These assumptions include:\n",
    "\n",
    "=> Linearity: The relationship between the dependent variable and the independent variable(s) is linear.\n",
    "=> Independence: The observations are independent of each other.\n",
    "=> Homoscedasticity: The variance of the residuals is constant across all levels of the independent variable(s).\n",
    "=> Normality: The residuals follow a normal distribution.\n",
    "=> No multicollinearity: There is no high correlation between the independent variables.\n",
    "\n",
    "To check whether these assumptions hold in a given dataset, we can perform the following tests:\n",
    "\n",
    "=> Linearity: We can plot the dependent variable against each independent variable and look for a linear \n",
    "relationship. A scatter plot can be used to check the linearity assumption.\n",
    "=> Independence: We can check for independence by verifying that the data is collected randomly or at least \n",
    "through a well-designed sampling method. Also, we can check for serial correlation in the residuals using \n",
    "autocorrelation plots.\n",
    "=> Homoscedasticity: We can plot the residuals against the fitted values and check for any patterns in the plot. \n",
    "If the variance of the residuals is constant across all levels of the independent variable(s), we can conclude \n",
    "that the homoscedasticity assumption holds.\n",
    "=> Normality: We can plot the histogram or the normal probability plot of the residuals and check whether they \n",
    "follow a normal distribution. If the residuals are normally distributed, we can conclude that the normality \n",
    "assumption holds.\n",
    "=> No multicollinearity: We can calculate the correlation matrix of the independent variables and check for high \n",
    "correlation values. If the correlation between the independent variables is high, it may indicate multicollinearity.\n",
    "\n",
    "In conclusion, checking for the assumptions of linear regression is an essential step in analyzing the data using \n",
    "this method. By verifying these assumptions, we can ensure that the results obtained from linear regression \n",
    "analysis are reliable and accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efbba13-3614-4648-8f19-fc95cd27810f",
   "metadata": {},
   "source": [
    "### Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60493d2b-093b-4080-85c6-c93c3691813b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf715500-f9fc-4170-87fe-1dbcad29a34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:- In linear regression, the slope and intercept are two parameters that describe the relationship between the \n",
    "dependent variable and the independent variable(s). The slope represents the change in the dependent variable for \n",
    "a unit change in the independent variable, while the intercept represents the value of the dependent variable when\n",
    "the independent variable is zero.\n",
    "\n",
    "More formally, the slope and intercept of a linear regression model are estimated from the data using the method\n",
    "of least squares. The slope is denoted by the symbol β1, while the intercept is denoted by β0. The linear\n",
    "regression equation can be written as:\n",
    "\n",
    "Y = β0 + β1*X + ε\n",
    "\n",
    "where Y is the dependent variable, X is the independent variable, ε is the error term, β0 is the intercept, and β1 \n",
    "is the slope.\n",
    "\n",
    "To interpret the slope and intercept in a linear regression model, we can use the following examples:\n",
    "\n",
    "Example: Suppose we want to determine the relationship between the number of hours a student studies per day and \n",
    "their exam score. We collect data on 20 students and fit a linear regression model to the data. The estimated \n",
    "equation is:\n",
    "\n",
    "Exam score = 60 + 5*Number of study hours + ε\n",
    "\n",
    "Interpretation: The intercept (60) represents the expected exam score for a student who does not study (i.e., the\n",
    "exam score when the number of study hours is zero). The slope (5) represents the expected increase in the exam \n",
    "score for every additional hour of study.\n",
    "\n",
    "For example, if a student studies for 3 hours a day, the expected exam score is:\n",
    "\n",
    "Exam score = 60 + 5*3 = 75\n",
    "\n",
    "If the student studies for 4 hours a day, the expected exam score is:\n",
    "\n",
    "Exam score = 60 + 5*4 = 80\n",
    "\n",
    "In conclusion, the slope and intercept of a linear regression model provide valuable information about the\n",
    "relationship between the dependent variable and the independent variable(s). By interpreting these parameters, we\n",
    "can make predictions and draw conclusions about the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4978fe3b-95e8-480b-aad2-70ae88869779",
   "metadata": {},
   "source": [
    "### Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11357714-6cb3-4fc4-ad84-e491e235ae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88266c49-62b0-47ee-ad18-cf42b9df7fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:- Gradient descent is an optimization algorithm used to find the minimum value of a function by iteratively \n",
    "adjusting its parameters. In machine learning, gradient descent is used to update the parameters of a model to \n",
    "minimize the error between the predicted and actual values.\n",
    "\n",
    "The basic idea behind gradient descent is to move in the direction of the negative gradient of the function, as \n",
    "the gradient points in the direction of the steepest ascent. By moving in the opposite direction, we can eventually\n",
    "reach the minimum value of the function.\n",
    "\n",
    "The process of gradient descent can be summarized as follows:\n",
    "\n",
    "=> Initialize the parameters of the model to some random values.\n",
    "=> Calculate the gradient of the loss function with respect to the parameters.\n",
    "=> Update the parameters by subtracting a fraction (the learning rate) of the gradient from the current values.\n",
    "=> Repeat steps 2 and 3 until the model converges to a minimum value of the loss function.\n",
    "\n",
    "There are two main types of gradient descent: batch gradient descent and stochastic gradient descent. In batch \n",
    "gradient descent, the gradient is calculated using the entire training dataset, while in stochastic gradient \n",
    "descent, the gradient is calculated using a single example at a time. Mini-batch gradient descent is a variant \n",
    "that uses a small subset of the training data to calculate the gradient.\n",
    "\n",
    "Gradient descent is a powerful optimization algorithm used in many machine learning models, including linear \n",
    "regression, logistic regression, and neural networks. By minimizing the error between the predicted and actual \n",
    "values, gradient descent can help the model learn the underlying patterns in the data and make accurate predictions\n",
    "on new examples. However, it is important to choose an appropriate learning rate and number of iterations to \n",
    "ensure that the model converges to the optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10683624-ac78-4758-9799-917876693b05",
   "metadata": {},
   "source": [
    "### Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752455e3-1b77-4353-84eb-ee7a23bb5304",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdfe28f-cf17-4a31-a375-3b96b03d876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:- Multiple linear regression is a statistical technique used to model the relationship between a dependent \n",
    "variable and two or more independent variables. It is an extension of simple linear regression, which only \n",
    "considers one independent variable.\n",
    "\n",
    "The multiple linear regression model can be represented as follows:\n",
    "\n",
    "y = β0 + β1x1 + β2x2 + ... + βpxp + ε\n",
    "\n",
    "where y is the dependent variable, x1, x2, ..., xp are the independent variables, β0 is the intercept, β1, β2, ...,\n",
    "βp are the coefficients for the independent variables, and ε is the error term.\n",
    "\n",
    "In multiple linear regression, the goal is to estimate the coefficients β1, β2, ..., βp that minimize the sum of \n",
    "the squared residuals (the difference between the predicted and actual values). The coefficients represent the \n",
    "change in the dependent variable associated with a unit change in the corresponding independent variable, while \n",
    "holding all other independent variables constant.\n",
    "\n",
    "The multiple linear regression model differs from simple linear regression in several ways:\n",
    "\n",
    "=> Number of independent variables: Simple linear regression considers only one independent variable, while\n",
    "multiple linear regression considers two or more independent variables.\n",
    "\n",
    "=> Interpretation of coefficients: In simple linear regression, the coefficient represents the change in the \n",
    "dependent variable for a unit change in the independent variable. In multiple linear regression, the coefficients\n",
    "represent the change in the dependent variable for a unit change in the corresponding independent variable, while\n",
    "holding all other independent variables constant.\n",
    "\n",
    "=> Model complexity: Multiple linear regression models are more complex than simple linear regression models, as\n",
    "they consider multiple independent variables. As the number of independent variables increases, the model becomes \n",
    "more complex and can be more difficult to interpret.\n",
    "\n",
    "Overall, multiple linear regression is a powerful technique for modeling the relationship between a dependent \n",
    "variable and multiple independent variables. By estimating the coefficients of the independent variables, the \n",
    "model can provide insights into the factors that influence the dependent variable and make accurate predictions on\n",
    "new examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da6fdb5-8b43-4ff2-9cdf-cefec824efee",
   "metadata": {},
   "source": [
    "### Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0528e2-96ec-4c69-815a-4b1850b5a6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3506a8-c9cc-4ff2-ade2-bc15e5040eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:- Multicollinearity is a common problem in multiple linear regression, which occurs when two or more \n",
    "independent variables in the model are highly correlated with each other. This can cause problems in the estimation\n",
    "of the regression coefficients and can make it difficult to interpret the results.\n",
    "\n",
    "The presence of multicollinearity can be detected using several methods, including:\n",
    "\n",
    "=> Correlation matrix: Calculate the correlation matrix between the independent variables. If the correlation \n",
    "coefficient between two variables is greater than 0.7, it is an indication of multicollinearity.\n",
    "\n",
    "=> Variance Inflation Factor (VIF): The VIF measures the extent to which the variance of an estimated regression\n",
    "coefficient is increased due to multicollinearity in the model. A VIF value greater than 5 or 10 is generally \n",
    "considered an indication of multicollinearity.\n",
    "\n",
    "To address the issue of multicollinearity, there are several techniques that can be used:\n",
    "\n",
    "=> Remove one of the highly correlated variables: If two or more independent variables are highly correlated, one\n",
    "of them can be removed from the model.\n",
    "\n",
    "=> Combine the highly correlated variables: If the highly correlated variables represent similar concepts, they can\n",
    "be combined into a single variable.\n",
    "\n",
    "=> Use regularization techniques: Regularization techniques such as Ridge regression or Lasso regression can be \n",
    "used to reduce the impact of multicollinearity on the regression coefficients.\n",
    "\n",
    "=> Collect more data: Collecting more data can help to reduce the impact of multicollinearity by increasing the \n",
    "sample size and reducing the correlation between the independent variables.\n",
    "\n",
    "In summary, multicollinearity is a common issue in multiple linear regression, which can cause problems in the \n",
    "estimation of the regression coefficients and the interpretation of the results. Detecting multicollinearity and \n",
    "addressing it appropriately is essential for building accurate and reliable regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44608aa-7e5c-4897-a97f-29b4f8eced78",
   "metadata": {},
   "source": [
    "### Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd42528-2ea4-4dc3-b1af-d5f9b1b4d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc7338-15f9-4fb5-9c62-05c247153852",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:- Polynomial regression is a type of regression analysis that models the relationship between a dependent \n",
    "variable and one independent variable by fitting a polynomial function to the data. Polynomial regression is an \n",
    "extension of linear regression, which models the relationship between the dependent variable and an independent \n",
    "variable using a linear function.\n",
    "\n",
    "The polynomial regression model can be represented as follows:\n",
    "\n",
    "y = β0 + β1x + β2x^2 + ... + βpx^p + ε\n",
    "\n",
    "where y is the dependent variable, x is the independent variable, β0 is the intercept, β1, β2, ..., βp are the \n",
    "coefficients for the independent variables, x^2, x^3, ..., x^p are the higher-order terms of the independent\n",
    "variable, and ε is the error term.\n",
    "\n",
    "In polynomial regression, the goal is to estimate the coefficients β1, β2, ..., βp that minimize the sum of the \n",
    "squared residuals (the difference between the predicted and actual values). The coefficients represent the change \n",
    "in the dependent variable associated with a unit change in the corresponding independent variable, while accounting\n",
    "for the higher-order terms.\n",
    "\n",
    "The polynomial regression model differs from linear regression in several ways:\n",
    "\n",
    "=> Functional form: In linear regression, the relationship between the dependent and independent variables is\n",
    "modeled using a linear function. In polynomial regression, the relationship is modeled using a polynomial function,\n",
    "which can capture more complex patterns in the data.\n",
    "\n",
    "=> Degree of polynomial: In polynomial regression, the degree of the polynomial function (the highest power of the \n",
    "independent variable) is a parameter that can be adjusted to fit the data. A higher degree polynomial can fit the\n",
    "data more closely, but can also lead to overfitting.\n",
    "\n",
    "=> Interpretation of coefficients: In polynomial regression, the coefficients represent the change in the dependent\n",
    "variable associated with a unit change in the independent variable and the higher-order terms. The interpretation \n",
    "of these coefficients can be more complex than in linear regression.\n",
    "\n",
    "Overall, polynomial regression is a powerful technique for modeling nonlinear relationships between a dependent \n",
    "variable and an independent variable. By fitting a polynomial function to the data, it can capture more complex \n",
    "patterns and make more accurate predictions than linear regression. However, it is important to carefully select \n",
    "the degree of the polynomial to avoid overfitting and interpret the coefficients appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac02c52-8845-4bea-a1f7-4e45f98f45ad",
   "metadata": {},
   "source": [
    "### Q8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cee156-8885-4cad-9086-a7da7815f475",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5484103b-23d4-4bb0-b855-362bb3667e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:- Advantages of Polynomial Regression:\n",
    "\n",
    "=> It can model nonlinear relationships between the independent and dependent variables, which cannot be captured \n",
    "by linear regression.\n",
    "=> It can fit a wide range of curves and can be used to approximate any function with a high degree of accuracy.\n",
    "=> It can provide a better fit to the data and improve the accuracy of the predictions.\n",
    "=> It can be used to identify the optimal degree of the polynomial by comparing the goodness-of-fit measures, such\n",
    "as the R-squared and adjusted R-squared.\n",
    "\n",
    "Disadvantages of Polynomial Regression:\n",
    "\n",
    "=> It can overfit the data, especially when the degree of the polynomial is too high, leading to poor \n",
    "generalization to new data.\n",
    "=> It can be difficult to interpret the coefficients, especially when the degree of the polynomial is high.\n",
    "=> It can be computationally expensive to fit the model, especially when the degree of the polynomial is high.\n",
    "Polynomial regression is useful in situations where the relationship between the dependent and independent \n",
    "variables is nonlinear and cannot be captured by linear regression. It can also be used to improve the accuracy of\n",
    "the predictions when the data shows a nonlinear trend. However, it is important to carefully select the degree of \n",
    "the polynomial to avoid overfitting and interpret the coefficients appropriately. Polynomial regression is often\n",
    "used in fields such as finance, physics, and engineering, where nonlinear relationships are common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2d39cc-7cea-4ae2-8260-863e4714731c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
