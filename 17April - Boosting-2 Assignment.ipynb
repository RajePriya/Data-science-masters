{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29484c46",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61cfd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82984a02",
   "metadata": {},
   "source": [
    "Ans:-\n",
    "    \n",
    "    Gradient Boosting Regression is a machine learning technique used for solving regression problems, where the goal is to predict a continuous numerical output. It is a variant of the Gradient Boosting algorithm, which is also commonly used for classification tasks.\n",
    "\n",
    "In Gradient Boosting Regression, the algorithm builds an ensemble of weak regression models (typically decision trees) sequentially to create a strong predictive model. The algorithm iteratively learns from the mistakes of the previously built weak learners and aims to minimize the residual errors in the predictions.\n",
    "\n",
    "The working of Gradient Boosting Regression can be summarized in the following steps:\n",
    "\n",
    "1. Initialization:\n",
    "\n",
    "- The process begins by fitting a simple regression model to the training data, which serves as the first weak learner in the ensemble. The initial predictions are often set to the mean (or another appropriate value) of the target variable.\n",
    "2. Residual Calculation:\n",
    "\n",
    "- The algorithm calculates the difference between the actual target values and the predictions made by the current ensemble model. These differences are known as the residuals.\n",
    "3. Training Weak Learners (Decision Trees):\n",
    "\n",
    "- A new weak learner (typically a decision tree with limited depth) is trained to predict the residuals from the previous step.\n",
    "- The new weak learner aims to learn the patterns in the residuals and minimize the errors in predicting the remaining variance not explained by the current ensemble.\n",
    "4. Update Ensemble Predictions:\n",
    "\n",
    "- The predictions of the new weak learner are combined with the predictions of the existing ensemble to update the overall predictions.\n",
    "- The predictions from all weak learners are aggregated using a learning rate (also known as shrinkage), which controls the contribution of each weak learner to the final prediction.\n",
    "5. Iterative Process:\n",
    "\n",
    "- Steps 2 to 4 are repeated for a predetermined number of iterations or until a specified performance metric reaches a satisfactory level.\n",
    "- In each iteration, a new weak learner is trained to predict the negative gradient (residuals) of the loss function with respect to the current ensemble predictions.\n",
    "6. Final Prediction:\n",
    "\n",
    "- The final prediction is made by summing the predictions from all weak learners, weighted by the learning rate, to obtain the overall prediction of the Gradient Boosting Regression model.\n",
    "\n",
    "\n",
    "Gradient Boosting Regression is powerful and flexible, capable of capturing complex relationships between features and the target variable. It tends to handle outliers well and can achieve high predictive accuracy. However, like other ensemble methods, it can be computationally intensive and may require tuning of hyperparameters to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866c7f17",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3e5f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42c78ac",
   "metadata": {},
   "source": [
    "Ans:-\n",
    "    \n",
    "    Implementing a full-fledged gradient boosting algorithm from scratch can be quite involved. Instead, I'll provide a simplified version of the algorithm using Python and NumPy to demonstrate the basic steps. We'll use a simple regression problem and create a weak learner as a decision tree with one split (stump). Note that this simplified implementation is for illustrative purposes and may not be as efficient or robust as mature libraries like scikit-learn's GradientBoostingRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "559196e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.032268159756570255\n",
      "R-squared: 0.9838659201217149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionTreeStump:\n",
    "    def __init__(self):\n",
    "        self.feature_index = None\n",
    "        self.threshold = None\n",
    "        self.left_value = None\n",
    "        self.right_value = None\n",
    "\n",
    "    def fit(self, X, y, sample_weights):\n",
    "        n_samples, n_features = X.shape\n",
    "        best_mse = np.inf\n",
    "\n",
    "        for feature_idx in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature_idx] <= threshold\n",
    "                left_value = np.mean(y[left_mask])\n",
    "                right_value = np.mean(y[~left_mask])\n",
    "                mse = np.sum(sample_weights * (y - (left_value * left_mask + right_value * ~left_mask)) ** 2)\n",
    "\n",
    "                if mse < best_mse:\n",
    "                    self.feature_index = feature_idx\n",
    "                    self.threshold = threshold\n",
    "                    self.left_value = left_value\n",
    "                    self.right_value = right_value\n",
    "                    best_mse = mse\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.where(X[:, self.feature_index] <= self.threshold, self.left_value, self.right_value)\n",
    "\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.estimators = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        sample_weights = np.ones(n_samples) / n_samples\n",
    "        y_pred = np.zeros(n_samples)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            residual = y - y_pred\n",
    "            estimator = DecisionTreeStump()\n",
    "            estimator.fit(X, residual, sample_weights)\n",
    "\n",
    "            y_pred += self.learning_rate * estimator.predict(X)\n",
    "            self.estimators.append(estimator)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "        for estimator in self.estimators:\n",
    "            y_pred += self.learning_rate * estimator.predict(X)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# Example dataset\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2, 4, 3, 5, 6])\n",
    "\n",
    "# Create and train the gradient boosting regressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "gb_regressor.fit(X, y)\n",
    "\n",
    "# Make predictions on the training data\n",
    "y_pred = gb_regressor.predict(X)\n",
    "\n",
    "# Evaluate performance using mean squared error and R-squared\n",
    "mse = np.mean((y - y_pred) ** 2)\n",
    "r_squared = 1 - np.sum((y - y_pred) ** 2) / np.sum((y - np.mean(y)) ** 2)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaba27d3",
   "metadata": {},
   "source": [
    "This implementation provides a basic outline of gradient boosting for regression using a simple decision tree stump as a weak learner. Keep in mind that this is a simplified version and may not handle all scenarios encountered in real-world applications. In practice, you can use libraries like scikit-learn or XGBoost, which offer efficient and optimized implementations of gradient boosting for regression and classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cc8a67",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0880f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad23c75",
   "metadata": {},
   "source": [
    "Ans:-\n",
    "    \n",
    "    To experiment with different hyperparameters and optimize the performance of the model, we can use grid search or random search to explore different combinations of hyperparameter values. In this example, I'll demonstrate how to use scikit-learn's GridSearchCV to perform grid search for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06d70bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn) (1.21.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cd90634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.032268159756570255\n",
      "R-squared: 0.9838659201217149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "class DecisionTreeStump:\n",
    "    def __init__(self):\n",
    "        self.feature_index = None\n",
    "        self.threshold = None\n",
    "        self.left_value = None\n",
    "        self.right_value = None\n",
    "\n",
    "    def fit(self, X, y, sample_weights):\n",
    "        n_samples, n_features = X.shape\n",
    "        best_mse = np.inf\n",
    "\n",
    "        for feature_idx in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature_idx] <= threshold\n",
    "                left_value = np.mean(y[left_mask])\n",
    "                right_value = np.mean(y[~left_mask])\n",
    "                mse = np.sum(sample_weights * (y - (left_value * left_mask + right_value * ~left_mask)) ** 2)\n",
    "\n",
    "                if mse < best_mse:\n",
    "                    self.feature_index = feature_idx\n",
    "                    self.threshold = threshold\n",
    "                    self.left_value = left_value\n",
    "                    self.right_value = right_value\n",
    "                    best_mse = mse\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.where(X[:, self.feature_index] <= self.threshold, self.left_value, self.right_value)\n",
    "\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.estimators = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        sample_weights = np.ones(n_samples) / n_samples\n",
    "        y_pred = np.zeros(n_samples)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            residual = y - y_pred\n",
    "            estimator = DecisionTreeStump()\n",
    "            estimator.fit(X, residual, sample_weights)\n",
    "\n",
    "            y_pred += self.learning_rate * estimator.predict(X)\n",
    "            self.estimators.append(estimator)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "        for estimator in self.estimators:\n",
    "            y_pred += self.learning_rate * estimator.predict(X)\n",
    "        return y_pred\n",
    "\n",
    "# Example dataset\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2, 4, 3, 5, 6])\n",
    "\n",
    "# Create and train the gradient boosting regressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "gb_regressor.fit(X, y)\n",
    "\n",
    "# Make predictions on the training data\n",
    "y_pred = gb_regressor.predict(X)\n",
    "\n",
    "# Evaluate performance using mean squared error and R-squared\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r_squared = r2_score(y, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8b3758",
   "metadata": {},
   "source": [
    "The code includes the definition of the fit and predict methods in the DecisionTreeStump and GradientBoostingRegressor classes, respectively, allowing you to run the gradient boosting algorithm with hyperparameter tuning using grid search or random search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7eead9",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa6c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb14fbf",
   "metadata": {},
   "source": [
    "Ans:-\n",
    "    \n",
    "    In the context of Gradient Boosting, a weak learner refers to a simple and relatively low-complexity model that performs only slightly better than random guessing on a given classification or regression task. Weak learners are often decision trees with limited depth (also known as decision stumps), linear models with limited features, or simple rules based on thresholding.\n",
    "\n",
    "The key characteristic of a weak learner is that its performance is just slightly better than chance on the training data. It means that the weak learner's accuracy is only slightly above 50% in the case of binary classification (or only slightly better than random predictions for regression problems).\n",
    "\n",
    "While weak learners alone might not be capable of producing accurate predictions, they become valuable in the context of Gradient Boosting. Gradient Boosting is an ensemble learning technique that combines multiple weak learners to create a strong predictive model. The weak learners' combination helps to correct the errors made by each other and improve the model's overall predictive performance.\n",
    "\n",
    "In each boosting iteration, a new weak learner is added to the ensemble, and it is trained on the weighted errors (residuals) of the previous learners. This process allows the new weak learner to focus on the instances where the current ensemble has made mistakes and adapt to those challenging cases. The subsequent weak learners continuously refine the model's predictions, gradually reducing both bias and variance, leading to a strong learner with improved generalization capabilities.\n",
    "\n",
    "The power of Gradient Boosting lies in its ability to sequentially add weak learners to form a powerful ensemble that can capture complex relationships in the data. By combining multiple weak learners, Gradient Boosting becomes a powerful machine learning algorithm capable of achieving high predictive accuracy and robustness in various tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3f3e9d",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea228b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b349e6",
   "metadata": {},
   "source": [
    "Ans:-\n",
    "    \n",
    "    The intuition behind the Gradient Boosting algorithm can be understood through the following steps:\n",
    "\n",
    "1. Start with a Weak Learner: The process begins by training a weak learner (e.g., decision tree with limited depth) on the training data. This weak learner is usually not very accurate on its own and may have high bias or variance.\n",
    "\n",
    "2. Focus on Misclassified Examples: In each boosting iteration, the algorithm focuses on the examples that the current ensemble of weak learners has misclassified or has significant errors. It assigns higher weights to these misclassified examples to make them more influential in the subsequent training.\n",
    "\n",
    "3. Train New Weak Learner: The next step is to train a new weak learner (another decision tree with limited depth) on the updated dataset, where the weights of the examples reflect their importance based on the errors made by the current ensemble.\n",
    "\n",
    "4. Weighted Voting: The predictions of all weak learners (including the new one) are then combined through a weighted voting scheme. The weight of each weak learner's prediction depends on its accuracy in the current iteration. More accurate learners get higher weights.\n",
    "\n",
    "5. Update Ensemble Prediction: After weighted voting, the ensemble prediction is updated by adding the predictions of all weak learners, weighted by their importance. This ensemble prediction becomes the improved prediction for the next boosting iteration.\n",
    "\n",
    "6. Iterative Process: Steps 2 to 5 are repeated for a predetermined number of iterations or until a specified performance metric reaches a satisfactory level. In each iteration, a new weak learner is added to the ensemble, and the model keeps learning from its mistakes.\n",
    "\n",
    "7. Final Prediction: Once all iterations are completed, the final prediction is made by combining the predictions of all the weak learners with their corresponding weights. The final prediction is the weighted sum (or average) of the predictions made by each weak learner.\n",
    "\n",
    "\n",
    "The intuition behind the Gradient Boosting algorithm lies in its ability to sequentially add weak learners and focus on the misclassified examples. By doing so, it gradually improves the model's predictions, reducing both bias and variance. The process of iteratively learning from the errors of the previous weak learners leads to a strong predictive model with improved generalization capabilities.\n",
    "\n",
    "Gradient Boosting is powerful because it leverages the strengths of ensemble learning, combining the complementary knowledge of multiple weak learners to create a robust and accurate model. Each new weak learner in the ensemble learns to correct the mistakes made by the previous ones, leading to a highly adaptive and flexible model. This adaptability makes Gradient Boosting suitable for a wide range of tasks and has contributed to its popularity in machine learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db173efa",
   "metadata": {},
   "source": [
    "# Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67dcbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c67bfdf",
   "metadata": {},
   "source": [
    "Ans:-\n",
    "    \n",
    "    \n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners (e.g., decision trees with limited depth) in a sequential and adaptive manner. The process involves iteratively adding weak learners to the ensemble, with each new learner focusing on the errors made by the previous ones. The steps to build the ensemble of weak learners can be summarized as follows:\n",
    "\n",
    "1. Start with Initial Prediction: The process begins by initializing the ensemble with a simple model that makes the initial prediction. Typically, the initial prediction is set to the average (or another suitable value) of the target variable for regression tasks or the log-odds for binary classification tasks.\n",
    "\n",
    "2. Compute Residuals: After the initial prediction, the algorithm computes the residuals by subtracting the predicted values from the true target values for each training example. The residuals represent the errors made by the initial model on the training data.\n",
    "\n",
    "3. Train First Weak Learner: A new weak learner (e.g., decision tree stump) is trained to predict the residuals obtained in the previous step. The weak learner aims to learn the patterns in the residuals and minimize the remaining errors not explained by the current ensemble.\n",
    "\n",
    "4. Weighted Voting: Once the first weak learner is trained, its predictions are combined with the predictions of the previous model using a weighted voting scheme. The weight assigned to each weak learner's prediction depends on its accuracy in the current iteration. More accurate learners get higher weights.\n",
    "\n",
    "5. Update Ensemble Prediction: The ensemble prediction is updated by adding the predictions of all weak learners, weighted by their importance. This ensemble prediction becomes the improved prediction for the next boosting iteration.\n",
    "\n",
    "6. Repeat the Process: Steps 2 to 5 are repeated for a pre-defined number of iterations (controlled by the n_estimators hyperparameter) or until a specified performance metric reaches a satisfactory level. In each iteration, a new weak learner is added to the ensemble, and the model keeps learning from its mistakes.\n",
    "\n",
    "7. Final Prediction: Once all iterations are completed, the final prediction is made by combining the predictions of all the weak learners with their corresponding weights. The final prediction is the weighted sum (or average) of the predictions made by each weak learner.\n",
    "\n",
    "The process of iteratively adding weak learners allows the Gradient Boosting algorithm to gradually improve the model's predictions. By focusing on the errors made by the previous learners, each new weak learner contributes to correcting the mistakes of the ensemble and capturing complex patterns in the data. This sequential nature of building the ensemble is what makes Gradient Boosting a powerful technique for supervised learning tasks, providing high predictive accuracy and robustness in various domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b07086b",
   "metadata": {},
   "source": [
    "# Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439f451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16432ead",
   "metadata": {},
   "source": [
    "Ans:-\n",
    "    \n",
    "    Constructing the mathematical intuition of the Gradient Boosting algorithm involves understanding the key mathematical concepts and equations that underpin its working. Here are the main steps involved in developing the mathematical intuition of Gradient Boosting:\n",
    "\n",
    "1. Loss Function: The first step is to define a loss function that quantifies the difference between the model's predictions and the true target values. For regression tasks, the typical loss function used is the mean squared error (MSE), while for binary classification, the binary cross-entropy loss (log loss) is commonly employed.\n",
    "\n",
    "2. Gradient of the Loss Function: The next step is to compute the gradient of the loss function with respect to the model's predictions. The gradient indicates the direction and magnitude of the steepest increase in the loss function concerning the model's predictions. For regression, the gradient of the MSE is simply the difference between the predicted value and the true target value. For binary classification, the gradient of the log loss involves more complex derivatives.\n",
    "\n",
    "3. Initialize the Ensemble: The process begins by initializing the ensemble with a simple model (e.g., the average for regression or the log-odds for binary classification). This initial model's predictions serve as the starting point for the iterative process.\n",
    "\n",
    "4. Compute Residuals: After obtaining the initial model's predictions, the algorithm calculates the residuals by subtracting the predictions from the true target values. The residuals represent the errors made by the current ensemble on the training data.\n",
    "\n",
    "5. Train Weak Learner to Fit Residuals: The next step is to train a new weak learner (e.g., decision tree stump) to predict the residuals obtained in the previous step. The weak learner's goal is to learn the patterns in the residuals and minimize the remaining errors not explained by the current ensemble.\n",
    "\n",
    "6. Compute Step Size (Learning Rate): The algorithm introduces a hyperparameter called the learning rate. The learning rate controls the contribution of each weak learner to the ensemble. A smaller learning rate means that each weak learner's impact on the ensemble is more gradual, reducing the risk of overfitting.\n",
    "\n",
    "7. Update Ensemble Predictions: The predictions of the new weak learner are combined with the predictions of the previous model using the step size (learning rate). The update involves adding the learning rate times the predictions of the new weak learner to the current ensemble predictions.\n",
    "\n",
    "8. Iterative Process: Steps 4 to 7 are repeated for a pre-defined number of iterations (controlled by the n_estimators hyperparameter) or until a specified performance metric reaches a satisfactory level. In each iteration, a new weak learner is added to the ensemble, and the model learns from its mistakes.\n",
    "\n",
    "9. Final Prediction: Once all iterations are completed, the final prediction is made by combining the predictions of all the weak learners with their corresponding weights. The final prediction is the weighted sum (or average) of the predictions made by each weak learner.\n",
    "\n",
    "The mathematical intuition of the Gradient Boosting algorithm lies in its ability to iteratively fit weak learners to the residuals and update the ensemble predictions to gradually improve the model's performance. By minimizing the residuals at each iteration, the model learns from its errors and adapts to the complex patterns in the data, ultimately creating a strong predictive model with high accuracy and robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b27492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82289d75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
