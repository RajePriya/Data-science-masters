{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28699255",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797f937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67c8f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:- \n",
    "    \n",
    "    To find the probability that an employee is a smoker given that he/she uses the health insurance plan, we can use Bayes' theorem. Bayes' theorem allows us to update the probability of an event based on new information.\n",
    "\n",
    "Let's define the events:\n",
    "A: Employee uses the health insurance plan.\n",
    "B: Employee is a smoker.\n",
    "\n",
    "We are given the following probabilities:\n",
    "P(A) = Probability that an employee uses the health insurance plan = 0.70 (70%)\n",
    "P(B|A) = Probability that an employee is a smoker given that he/she uses the health insurance plan = 0.40 (40%)\n",
    "\n",
    "Now, we want to find P(B|A) = Probability that an employee is a smoker given that he/she uses the health insurance plan.\n",
    "\n",
    "Bayes' theorem states:\n",
    "\n",
    "P(B∣A)= \n",
    "P(A)\n",
    "P(A∣B)⋅P(B)\n",
    "​\n",
    " \n",
    "\n",
    "where:\n",
    "P(A|B) = Probability that an employee uses the health insurance plan given that he/she is a smoker. This is not given directly, but we can calculate it using the information we have.\n",
    "\n",
    "P(B) = Probability that an employee is a smoker. We don't know this directly, but we can calculate it using the given information.\n",
    "\n",
    "We need to find P(A|B) and P(B):\n",
    "\n",
    "P(A|B): Probability that an employee uses the health insurance plan given that he/she is a smoker.\n",
    "This information is not directly given, but we can find it using the following formula:\n",
    "\n",
    "P(A∣B)= \n",
    "P(B)\n",
    "P(A∩B)\n",
    "​\n",
    " \n",
    "where P(A ∩ B) = Probability that an employee uses the health insurance plan and is a smoker.\n",
    "\n",
    "P(B): Probability that an employee is a smoker.\n",
    "This is directly given: P(B) = 0.40 (40%).\n",
    "\n",
    "Let's start with step 1:\n",
    "P(A ∩ B) = Probability that an employee uses the health insurance plan and is a smoker.\n",
    "\n",
    "Using the information provided:\n",
    "P(A) = 0.70 (70%)\n",
    "P(B) = 0.40 (40%)\n",
    "\n",
    "We know that:\n",
    "\n",
    "P(A∩B)=P(B)⋅P(A∣B)\n",
    "=\n",
    "0.40\n",
    "\n",
    "P(A∩B)=0.40⋅P(A∣B)\n",
    "\n",
    "Now, we need to find P(A|B) to calculate P(A ∩ B).\n",
    "\n",
    "We can rearrange the above equation to solve for P(A|B):\n",
    "\n",
    "P(A∣B)= \n",
    "P(B)\n",
    "P(A∩B)\n",
    "​\n",
    " \n",
    "\n",
    "Substitute the given values:\n",
    "\n",
    "P(A∣B)= \n",
    "0.40\n",
    "0.40⋅P(A∣B)\n",
    "​\n",
    " \n",
    "\n",
    "Now, we can solve for P(A|B): =1\n",
    "P(A∣B)=1\n",
    "\n",
    "Now, we have:\n",
    "P(A|B) = 1 (100%)\n",
    "P(B) = 0.40 (40%)\n",
    "\n",
    "Finally, we can use Bayes' theorem to find P(B|A):\n",
    "\n",
    "P(B∣A)= \n",
    "P(A)\n",
    "P(A∣B)⋅P(B)\n",
    "​\n",
    "\n",
    "0.40\n",
    "0.70\n",
    "P(B∣A)= \n",
    "0.70\n",
    "1⋅0.40\n",
    "\n",
    "0.40\n",
    "0.70\n",
    "P(B∣A)= \n",
    "0.70\n",
    "0.40\n",
    "\n",
    "0.5714\n",
    "P(B∣A)≈0.5714\n",
    "\n",
    "So, the probability that an employee is a smoker given that he/she uses the health insurance plan is approximately 0.5714 or 57.14%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a1cec0",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d66fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495c86e2",
   "metadata": {},
   "source": [
    "Ans:- \n",
    "    \n",
    "    Bernoulli Naive Bayes and Multinomial Naive Bayes are both variants of the Naive Bayes algorithm, a probabilistic classification method based on Bayes' theorem. They are commonly used in text classification tasks and other discrete feature-based classification problems. However, they have differences in how they handle the input features and the underlying assumptions they make about the data.\n",
    "\n",
    "1. Bernoulli Naive Bayes:\n",
    "\n",
    "- Input Features: Bernoulli Naive Bayes is suitable for binary feature data, where each feature can take on one of two values (usually 0 or 1). It assumes that each feature is conditionally independent of others, given the class label.\n",
    "- Assumption: Bernoulli Naive Bayes assumes that the presence or absence of a particular feature is relevant to the classification task but does not consider the frequency or count of the features.\n",
    "- Use Case: It is commonly used in document classification tasks, where the presence or absence of specific words in a document is used for classification (e.g., spam detection).\n",
    "\n",
    "2. Multinomial Naive Bayes:\n",
    "- Input Features: Multinomial Naive Bayes is suitable for discrete feature data, where each feature represents a count or frequency (non-negative integers). It assumes that each feature is conditionally independent of others, given the class label.\n",
    "- Assumption: Unlike Bernoulli Naive Bayes, Multinomial Naive Bayes takes into account the frequency of features. It assumes that the frequencies of different features (e.g., word counts) are relevant for classification.\n",
    "- Use Case: It is commonly used in text classification tasks, where the frequency of words or other features in a document is used for classification (e.g., sentiment analysis or topic classification).\n",
    "\n",
    "\n",
    "In summary, the main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the type of input features they can handle and the underlying assumptions they make about the data. Bernoulli Naive Bayes is suitable for binary features, while Multinomial Naive Bayes is suitable for discrete feature data, such as word counts or frequencies. The choice between the two depends on the nature of the data and the specific requirements of the classification task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5f5ca7",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f75315",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd91b237",
   "metadata": {},
   "source": [
    "Ans:- \n",
    "    \n",
    "    Bernoulli Naive Bayes handles missing values in a straightforward manner, which is common to most Naive Bayes algorithms. When encountering a missing value for a particular feature during the classification process, Bernoulli Naive Bayes ignores that feature for the specific instance being classified.\n",
    "\n",
    "Here's a step-by-step explanation of how Bernoulli Naive Bayes handles missing values:\n",
    "\n",
    "1. Training Phase:\n",
    "\n",
    "- During the training phase, Bernoulli Naive Bayes estimates probabilities based on the occurrences of features (binary values: 0 or 1) in the training data for each class label.\n",
    "- If a data point in the training set has missing values for certain features, those features are merely excluded from the feature occurrence count for the corresponding class. In other words, missing values are treated as if the feature is not present in the data point.\n",
    "\n",
    "2. Classification Phase:\n",
    "- When classifying a new instance with missing values, the algorithm uses only the available features for that instance. Any missing features are ignored during classification.\n",
    "- The algorithm calculates the conditional probabilities of each class label based on the available features and applies Bayes' theorem to determine the most likely class for the instance.\n",
    "\n",
    "It's important to note that the Naive Bayes assumption of feature independence given the class label means that the absence of certain features (due to missing values) does not impact the independence assumption for the remaining features. This assumption simplifies the computations and makes Naive Bayes efficient even with missing data.\n",
    "\n",
    "While Bernoulli Naive Bayes handles missing values in this way, it's generally good practice to handle missing data appropriately during the preprocessing steps before applying any machine learning algorithm. Some common techniques for handling missing data include imputation (replacing missing values with estimated values based on other data points) or using algorithms that can handle missing data directly. However, these techniques are not specific to Bernoulli Naive Bayes and can be applied to other machine learning algorithms as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20283396",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e093f385",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed01f287",
   "metadata": {},
   "source": [
    "Ans:- \n",
    "    \n",
    "    Yes, Gaussian Naive Bayes can be used for multi-class classification. It is one of the variants of the Naive Bayes algorithm that is particularly suited for continuous data or data that can be assumed to follow a Gaussian (normal) distribution. Gaussian Naive Bayes can handle multiple classes in a classification problem.\n",
    "\n",
    "In multi-class classification, there are more than two classes, and the task is to assign an instance to one of those multiple classes based on its features. Each class has its own set of features and associated probabilities.\n",
    "\n",
    "The Gaussian Naive Bayes algorithm makes the following assumptions:\n",
    "\n",
    "1. The features within each class follow a Gaussian (normal) distribution.\n",
    "2. The features are conditionally independent of each other, given the class label.\n",
    "\n",
    "Given these assumptions, the algorithm estimates the mean and standard deviation of each feature for each class from the training data. Then, during the classification phase, it calculates the likelihood of an instance belonging to each class based on the Gaussian probability density function for each feature.\n",
    "\n",
    "The class with the highest likelihood (posterior probability) is chosen as the predicted class for the given instance.\n",
    "\n",
    "In the multi-class scenario, Gaussian Naive Bayes calculates the probabilities for each class separately and assigns the instance to the class with the highest probability. It performs well in situations where the assumptions of the algorithm hold reasonably well and the data features are continuous.\n",
    "\n",
    "However, it's essential to keep in mind that Naive Bayes, including Gaussian Naive Bayes, relies on strong independence assumptions between features, which may not always hold in real-world data. Despite its simplicity, Gaussian Naive Bayes can be surprisingly effective in certain situations, especially when the independence assumption is not significantly violated, or when the data distribution is approximately Gaussian-like. In practice, it is always a good idea to compare the performance of Gaussian Naive Bayes with other more complex algorithms on your specific dataset to determine the best model for your classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b6de0d",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371cdbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "\n",
    "Note: This dataset contains a binary classification problem with multiple features. The dataset is\n",
    "relatively small, but it can be used to demonstrate the performance of the different variants of Naive\n",
    "Bayes on a real-world problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b66295f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes:\n",
      "Accuracy:  0.8839380364047911\n",
      "Precision:  0.8869617393737383\n",
      "Recall:  0.8152389047416673\n",
      "F1 Score:  0.8481249015095276\n",
      "\n",
      "\n",
      "Multinomial Naive Bayes:\n",
      "Accuracy:  0.7863496180326323\n",
      "Precision:  0.7393175533565436\n",
      "Recall:  0.7214983911116508\n",
      "F1 Score:  0.7282909724016348\n",
      "\n",
      "\n",
      "Gaussian Naive Bayes:\n",
      "Accuracy:  0.8217730830896915\n",
      "Precision:  0.7103733928118492\n",
      "Recall:  0.9569516119239877\n",
      "F1 Score:  0.8130660909542995\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset without column names\n",
    "data = pd.read_csv(\"spambase.csv\", header=None)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Implement Bernoulli Naive Bayes\n",
    "bernoulli_nb = BernoulliNB()\n",
    "bernoulli_scores_accuracy = cross_val_score(bernoulli_nb, X, y, cv=10, scoring='accuracy')\n",
    "bernoulli_scores_precision = cross_val_score(bernoulli_nb, X, y, cv=10, scoring='precision')\n",
    "bernoulli_scores_recall = cross_val_score(bernoulli_nb, X, y, cv=10, scoring='recall')\n",
    "bernoulli_scores_f1 = cross_val_score(bernoulli_nb, X, y, cv=10, scoring='f1')\n",
    "\n",
    "# Implement Multinomial Naive Bayes\n",
    "multinomial_nb = MultinomialNB()\n",
    "multinomial_scores_accuracy = cross_val_score(multinomial_nb, X, y, cv=10, scoring='accuracy')\n",
    "multinomial_scores_precision = cross_val_score(multinomial_nb, X, y, cv=10, scoring='precision')\n",
    "multinomial_scores_recall = cross_val_score(multinomial_nb, X, y, cv=10, scoring='recall')\n",
    "multinomial_scores_f1 = cross_val_score(multinomial_nb, X, y, cv=10, scoring='f1')\n",
    "\n",
    "# Implement Gaussian Naive Bayes\n",
    "gaussian_nb = GaussianNB()\n",
    "gaussian_scores_accuracy = cross_val_score(gaussian_nb, X, y, cv=10, scoring='accuracy')\n",
    "gaussian_scores_precision = cross_val_score(gaussian_nb, X, y, cv=10, scoring='precision')\n",
    "gaussian_scores_recall = cross_val_score(gaussian_nb, X, y, cv=10, scoring='recall')\n",
    "gaussian_scores_f1 = cross_val_score(gaussian_nb, X, y, cv=10, scoring='f1')\n",
    "\n",
    "# Function to report performance metrics\n",
    "def report_metrics(name, accuracy, precision, recall, f1):\n",
    "    print(f\"{name} Naive Bayes:\")\n",
    "    print(\"Accuracy: \", accuracy.mean())\n",
    "    print(\"Precision: \", precision.mean())\n",
    "    print(\"Recall: \", recall.mean())\n",
    "    print(\"F1 Score: \", f1.mean())\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Report the performance metrics for each classifier\n",
    "report_metrics(\"Bernoulli\", bernoulli_scores_accuracy, bernoulli_scores_precision, bernoulli_scores_recall, bernoulli_scores_f1)\n",
    "report_metrics(\"Multinomial\", multinomial_scores_accuracy, multinomial_scores_precision, multinomial_scores_recall, multinomial_scores_f1)\n",
    "report_metrics(\"Gaussian\", gaussian_scores_accuracy, gaussian_scores_precision, gaussian_scores_recall, gaussian_scores_f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87309375",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "\n",
    "After running the above code, you will get the performance metrics (Accuracy, Precision, Recall, and F1 Score) for each variant of Naive Bayes using 10-fold cross-validation on the \"spambase\" dataset.\n",
    "\n",
    "The best variant of Naive Bayes depends on the specific dataset and the nature of the features. However, in general, Bernoulli Naive Bayes tends to perform well when the features are binary (presence or absence of features, as in spam classification). Multinomial Naive Bayes is suitable for discrete feature counts, while Gaussian Naive Bayes is used when features follow a Gaussian distribution (continuous data).\n",
    "\n",
    "To determine which variant performed the best, compare the results of all three variants and choose the one with the highest performance based on the evaluation metrics (Accuracy, Precision, Recall, and F1 Score).\n",
    "\n",
    "Limitations of Naive Bayes:\n",
    "\n",
    "1. Independence Assumption: Naive Bayes assumes that all features are conditionally independent given the class label, which might not hold in real-world scenarios.\n",
    "2. Sensitivity to Irrelevant Features: Naive Bayes can be sensitive to irrelevant or noisy features, which can impact its performance negatively.\n",
    "3. Zero Frequency Issue: In Bernoulli and Multinomial Naive Bayes, if a feature has a frequency of zero for a particular class, the conditional probability becomes zero, leading to a biased estimation.\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "\n",
    "In this analysis, we implemented and evaluated three variants of Naive Bayes classifiers (Bernoulli, Multinomial, and Gaussian) on the \"spambase\" dataset for a binary classification problem. We reported performance metrics such as Accuracy, Precision, Recall, and F1 Score using 10-fold cross-validation.\n",
    "\n",
    "Based on the results, we can determine which variant of Naive Bayes performed the best on this dataset. Additionally, we observed some limitations of Naive Bayes, such as the strong independence assumption and sensitivity to irrelevant features.\n",
    "\n",
    "For future work, we can explore techniques to handle the limitations of Naive Bayes, preprocess the data to improve feature relevance, and consider other classifiers like logistic regression, decision trees, or ensemble methods for comparison. Additionally, conducting hyperparameter tuning could further improve the performance of the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc504c78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
